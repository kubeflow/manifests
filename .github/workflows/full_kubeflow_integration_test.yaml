name: Full Kubeflow Integration Test

on:
  workflow_dispatch:  
  schedule:
    - cron: '0 0 * * 0'  
  pull_request:
    paths:
      - example/kustomization.yaml
      - .github/workflows/full_kubeflow_integration_test.yaml
      - apps/pipeline/upstream/**
      - apps/katib/**
      - apps/centraldashboard/**
      - apps/jupyter/**
      - apps/profiles/**
      - apps/tensorboard/**
      - apps/training-operator/**
      - apps/kserve/**
      - common/**
      - tests/gh-actions/**
  push:
    branches:
      - master
      - v*-branch
      - 'releases/**'

jobs:
  build:
    name: Full E2E Integration Test
    runs-on:
      labels: ubuntu-latest-16-cores
    env:
      KIND_CLUSTER_NAME: kubeflow

    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: Install Python dependencies
      run: |
        pip install pytest kubernetes kfp==2.11.0 kserve pytest-timeout pyyaml requests

    - name: Install kubectl and kustomize
      run: |
        # Install kubectl
        curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
        chmod +x kubectl
        sudo mv kubectl /usr/local/bin/
        
        # Install kustomize
        curl -s "https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh" | bash
        sudo mv kustomize /usr/local/bin/

    - name: Install Kind
      run: |
        curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-amd64
        chmod +x ./kind
        sudo mv ./kind /usr/local/bin/kind
        kind version

    - name: Create Kind Cluster (per README)
      run: |
        cat <<EOF | kind create cluster --name=${KIND_CLUSTER_NAME} --config=-
        kind: Cluster
        apiVersion: kind.x-k8s.io/v1alpha4
        nodes:
        - role: control-plane
          image: kindest/node:v1.32.0@sha256:c48c62eac5da28cdadcf560d1d8616cfa6783b58f0d94cf63ad1bf49600cb027
          kubeadmConfigPatches:
          - |
            kind: ClusterConfiguration
            apiServer:
              extraArgs:
                "service-account-issuer": "https://kubernetes.default.svc"
                "service-account-signing-key-file": "/etc/kubernetes/pki/sa.key"
        EOF
        
        # Save kubeconfig
        kind get kubeconfig --name=${KIND_CLUSTER_NAME} > /tmp/kubeflow-config
        export KUBECONFIG=/tmp/kubeflow-config
        echo "KUBECONFIG=/tmp/kubeflow-config" >> $GITHUB_ENV
        
        # Verify cluster is ready
        kubectl cluster-info
        kubectl get nodes

    - name: Configure Docker and Create Registry Secret
      run: |
        # Configure Docker
        echo "${{ secrets.DOCKER_TOKEN }}" | docker login -u "${{ secrets.DOCKER_USERNAME }}" --password-stdin
        
        # Create registry secret for image pulling
        # If running in GitHub actions, create from secrets
        kubectl create secret generic regcred \
          --from-file=.dockerconfigjson=$HOME/.docker/config.json \
          --type=kubernetes.io/dockerconfigjson
        
        # Verify secret was created
        kubectl get secret regcred

    - name: Create kubeflow namespace
      run: |
        kubectl create namespace kubeflow || true

    - name: Deploy Full Kubeflow Stack (per README)
      run: |
        # Apply with retry logic using exact command from README
        echo "Installing full Kubeflow stack from example/kustomization.yaml"
        while ! kustomize build example | kubectl apply --server-side --force-conflicts -f -; do 
          echo "Retrying to apply resources"; 
          sleep 20; 
        done
        
        # Wait for all pods to be ready
        echo "Waiting for all deployments to be ready..."
        kubectl wait --for=condition=available --timeout=1200s deployment --all -n cert-manager || true
        kubectl wait --for=condition=available --timeout=1200s deployment --all -n istio-system || true
        kubectl wait --for=condition=available --timeout=1200s deployment --all -n auth || true
        kubectl wait --for=condition=available --timeout=1800s deployment --all -n kubeflow || true
        
        # Check deployment status
        kubectl get deployments --all-namespaces
        kubectl get pods --all-namespaces

    - name: Wait for Critical Services
      run: |
        # Wait for Istio gateway
        echo "Waiting for Istio gateway..."
        kubectl wait --for=condition=ready --timeout=300s -n istio-system --all pods || true
        
        # Wait for cert-manager
        echo "Waiting for cert-manager..."
        kubectl wait --for=condition=ready --timeout=300s -n cert-manager --all pods || true
        
        # Wait for oauth2-proxy and dex (auth components)
        echo "Waiting for auth components..."
        kubectl wait --for=condition=ready --timeout=300s -n auth --all pods || true
        
        # Wait for metacontroller
        echo "Waiting for metacontroller..."
        kubectl wait --for=condition=ready --timeout=300s -n kubeflow --selector=app=metacontroller pods || true
        
        # Wait for central dashboard
        echo "Waiting for central dashboard..."
        kubectl wait --for=condition=ready --timeout=300s -n kubeflow --selector=app=centraldashboard pods || true
        
        # Wait for ML pipeline
        echo "Waiting for ML pipeline..."
        kubectl wait --for=condition=ready --timeout=300s -n kubeflow --selector=app=ml-pipeline pods || true
        
        # Wait for Katib
        echo "Waiting for Katib..."
        kubectl wait --for=condition=ready --timeout=300s -n kubeflow --selector=app=katib pods || true

    - name: Create KF Profile
      run: |
        # User namespace should already be created from the full deployment
        # Just need to wait for resources to be ready
        sleep 60 # wait for profile controller to create resources
        
        METACONTROLLER_POD=$(kubectl get pods -n kubeflow -o json | jq -r '.items[] | select(.metadata.name | startswith("metacontroller")) | .metadata.name')
        if [[ -z "$METACONTROLLER_POD" ]]; then
          echo "Error: metacontroller pod not found in kubeflow namespace."
          exit 1
        fi
        kubectl logs -n kubeflow "$METACONTROLLER_POD"
        
        PIPELINES_PROFILE_CONTROLLER_POD=$(kubectl get pods -n kubeflow -o json | jq -r '.items[] | select(.metadata.name | startswith("kubeflow-pipelines-profile-controller")) | .metadata.name')
        if [[ -z "$PIPELINES_PROFILE_CONTROLLER_POD" ]]; then
          echo "Error: kubeflow-pipelines-profile-controller pod not found in kubeflow namespace."
          exit 1
        fi
        kubectl logs -n kubeflow "$PIPELINES_PROFILE_CONTROLLER_POD"
        
        KF_PROFILE=kubeflow-user-example-com
        kubectl -n $KF_PROFILE get pods,configmaps,secrets
        
        if ! kubectl get secret mlpipeline-minio-artifact -n $KF_PROFILE > /dev/null 2>&1; then
          echo "Error: Secret mlpipeline-minio-artifact not found in namespace $KF_PROFILE"
          exit 1
        fi
        kubectl get secret mlpipeline-minio-artifact -n "$KF_PROFILE" -o json | jq -r '.data | keys[] as $k | "\($k): \(. | .[$k] | @base64d)"' | tr '\n' ' '

    - name: Port Forward Istio Gateway
      run: |
        ingress_gateway_service=$(kubectl get svc --namespace istio-system --selector="app=istio-ingressgateway" --output jsonpath='{.items[0].metadata.name}')
        nohup kubectl port-forward --namespace istio-system svc/${ingress_gateway_service} 8080:80 &
        while ! curl localhost:8080; do echo waiting for port-forwarding; sleep 1; done; echo port-forwarding ready

    - name: Test oauth2-proxy Auth
      if: always()
      continue-on-error: true
      run: |
        # Test auth redirect
        echo "Testing authentication redirect..."
        HTTP_STATUS=$(curl -s -o /dev/null -w "%{http_code}" http://localhost:8080/pipeline/)
        if [[ $HTTP_STATUS -eq 302 || $HTTP_STATUS -eq 401 ]]; then
          echo "Auth check successful: Got redirect or unauthorized as expected"
        else
          echo "Warning: Expected auth redirect, got HTTP status $HTTP_STATUS"
        fi

    - name: Run Pipeline Tests
      run: |
        KF_PROFILE=kubeflow-user-example-com
        
        # Test with authorized token
        TOKEN="$(kubectl -n $KF_PROFILE create token default-editor)"
        echo "Running pipeline with authorized token"
        python3 tests/gh-actions/pipeline_test.py run_pipeline "${TOKEN}" "${KF_PROFILE}"
        
        # Test with unauthorized token
        echo "Testing unauthorized access"
        TOKEN="$(kubectl -n default create token default)"
        python3 tests/gh-actions/pipeline_test.py test_unauthorized_access "${TOKEN}" "${KF_PROFILE}"
        echo "Test succeeded. Token from unauthorized ServiceAccount cannot list pipelines in $KF_PROFILE namespace."

    - name: Test Central Dashboard Access
      run: |
        # Test central dashboard is accessible
        curl -I http://localhost:8080/
        
        # Check other components
        echo "Checking Notebooks access"
        curl -I http://localhost:8080/jupyter/
        
        echo "Checking Pipelines access"
        curl -I http://localhost:8080/pipeline/
        
        echo "Checking KServe access"
        curl -I http://localhost:8080/models/
        
        echo "Checking Katib access"
        curl -I http://localhost:8080/katib/

    - name: Create Notebook Test
      if: always()
      run: |
        # Create a test notebook in the user namespace
        KF_PROFILE=kubeflow-user-example-com
        cat <<EOF > notebook-test.yaml
        apiVersion: kubeflow.org/v1
        kind: Notebook
        metadata:
          name: test-notebook
          namespace: ${KF_PROFILE}
        spec:
          template:
            spec:
              containers:
              - name: notebook
                image: jupyter/minimal-notebook:latest
                resources:
                  requests:
                    cpu: 500m
                    memory: 1Gi
                  limits:
                    cpu: 1
                    memory: 2Gi
        EOF
        
        kubectl apply -f notebook-test.yaml
        
        # Wait for notebook to be ready
        kubectl wait --for=condition=ready --timeout=300s -n ${KF_PROFILE} pod -l notebooks.kubeflow.org/notebook=test-notebook

    - name: Test Tensorboard Creation
      if: always()
      continue-on-error: true
      run: |
        # Create a tensorboard in user namespace
        KF_PROFILE=kubeflow-user-example-com
        cat <<EOF > tensorboard-test.yaml
        apiVersion: tensorboard.kubeflow.org/v1alpha1
        kind: Tensorboard
        metadata:
          name: test-tensorboard
          namespace: ${KF_PROFILE}
        spec:
          logspath: s3://fake-bucket/tensorboard-logs
        EOF
        
        kubectl apply -f tensorboard-test.yaml || echo "Tensorboard creation failed, continuing tests"
        
        # Check if tensorboard was created
        kubectl get tensorboards -n ${KF_PROFILE} || echo "No tensorboards found, continuing tests"

    - name: Test Katib Experiment
      if: always()
      continue-on-error: true
      run: |
        # Test if Katib is available
        if kubectl get crd experiments.kubeflow.org > /dev/null 2>&1; then
          # Create a simple Katib experiment
          KF_PROFILE=kubeflow-user-example-com
          cat <<EOF > katib-experiment.yaml
          apiVersion: kubeflow.org/v1beta1
          kind: Experiment
          metadata:
            name: test-katib-experiment
            namespace: ${KF_PROFILE}
          spec:
            objective:
              type: maximize
              goal: 0.99
              objectiveMetricName: Validation-accuracy
            algorithm:
              algorithmName: random
            parallelTrialCount: 1
            maxTrialCount: 1
            maxFailedTrialCount: 1
            parameters:
              - name: lr
                parameterType: double
                feasibleSpace:
                  min: "0.01"
                  max: "0.05"
            trialTemplate:
              primaryContainerName: training-container
              trialParameters:
                - name: learningRate
                  description: Learning rate
                  reference: lr
              trialSpec:
                apiVersion: batch/v1
                kind: Job
                spec:
                  template:
                    spec:
                      containers:
                        - name: training-container
                          image: docker.io/kubeflowkatib/mxnet-mnist:latest
                          command: ["python3", "/opt/mxnet-mnist/mnist.py", "--batch-size=64", "--lr=\${trialParameters.learningRate}"]
                      restartPolicy: Never
          EOF
          
          kubectl apply -f katib-experiment.yaml || echo "Katib experiment creation failed, continuing tests"
          
          # Check if experiment was created
          kubectl get experiments -n ${KF_PROFILE} || echo "No Katib experiments found, continuing tests"
        else
          echo "Katib CRD not found, skipping Katib tests"
        fi

    - name: Test Training Operator
      if: always()
      continue-on-error: true
      run: |
        # Test if Training Operator is available
        if kubectl get crd tfjobs.kubeflow.org > /dev/null 2>&1; then
          # Create a simple TFJob
          KF_PROFILE=kubeflow-user-example-com
          cat <<EOF > tfjob-test.yaml
          apiVersion: kubeflow.org/v1
          kind: TFJob
          metadata:
            name: test-tfjob
            namespace: ${KF_PROFILE}
          spec:
            tfReplicaSpecs:
              Worker:
                replicas: 1
                restartPolicy: OnFailure
                template:
                  spec:
                    containers:
                    - name: tensorflow
                      image: tensorflow/tensorflow:2.9.0
                      command:
                      - python
                      - -c
                      - |
                        import tensorflow as tf
                        print(tf.__version__)
                        print("TensorFlow training job completed!")
          EOF
          
          kubectl apply -f tfjob-test.yaml || echo "TFJob creation failed, continuing tests"
          
          # Check if job was created
          kubectl get tfjobs -n ${KF_PROFILE} || echo "No TFJobs found, continuing tests"
        else
          echo "Training Operator CRDs not found, skipping Training Operator tests"
        fi

    - name: Test KServe Model Deployment
      if: always()
      continue-on-error: true
      run: |
        # Only run if KServe is installed
        if kubectl get crd inferenceservices.serving.kserve.io > /dev/null 2>&1; then
          # Create a simple model deployment
          KF_PROFILE=kubeflow-user-example-com
          cat <<EOF > sklearn-test.yaml
          apiVersion: "serving.kserve.io/v1beta1"
          kind: "InferenceService"
          metadata:
            name: "sklearn-iris"
            namespace: "${KF_PROFILE}"
          spec:
            predictor:
              sklearn:
                storageUri: "gs://kfserving-examples/models/sklearn/iris"
          EOF
          
          kubectl apply -f sklearn-test.yaml
          
          # Wait for InferenceService to be ready - timeout after 2min
          kubectl wait --for=condition=ready --timeout=120s -n ${KF_PROFILE} isvc/sklearn-iris || echo "InferenceService did not become ready in time, continuing tests"
        else
          echo "KServe CRD not found, skipping KServe tests"
        fi

    - name: Verify All Components Running
      run: |
        # Check critical components are running
        kubectl get pods -n kubeflow
        
        # Check specific components from kustomization.yaml
        echo "Checking Cert-Manager status..."
        kubectl get pods -n cert-manager
        
        echo "Checking Istio status..."
        kubectl get pods -n istio-system
        
        echo "Checking Auth status (oauth2-proxy, dex)..."
        kubectl get pods -n auth
        
        echo "Checking KNative status..."
        kubectl get pods -n knative-serving || echo "KNative not deployed with dedicated namespace"
        
        echo "Checking Kubeflow components..."
        kubectl get deployment -n kubeflow centraldashboard || echo "Central Dashboard not found"
        kubectl get deployment -n kubeflow katib-controller || echo "Katib controller not found"
        kubectl get deployment -n kubeflow ml-pipeline || echo "Pipeline not found"
        kubectl get deployment -n kubeflow notebook-controller-deployment || echo "Notebook controller not found"
        kubectl get deployment -n kubeflow profiles-deployment || echo "Profiles not found"
        kubectl get deployment -n kubeflow tensorboard-controller-controller-manager || echo "Tensorboard controller not found"
        kubectl get deployment -n kubeflow training-operator || echo "Training operator not found"
        kubectl get deployment -n kubeflow kserve-controller-manager || echo "KServe not found"
        
        # Verify no pods are in failed state
        if kubectl get pods --all-namespaces | grep -E '(Error|CrashLoopBackOff)'; then
          echo "Found pods in failed state"
          exit 1
        fi
        
        echo "All Kubeflow components are running successfully"

    - name: Collect Logs on Failure
      if: failure()
      run: |
        mkdir -p logs
        
        # Collect resource status
        kubectl get all --all-namespaces > logs/all-resources.txt
        
        # Collect events
        kubectl get events --all-namespaces --sort-by=.metadata.creationTimestamp > logs/all-events.txt
        
        # Collect CRD status for all Kubeflow components
        echo "Collecting CRD status..."
        kubectl get crds | grep kubeflow > logs/kubeflow-crds.txt
        kubectl get crds | grep istio > logs/istio-crds.txt
        kubectl get crds | grep knative > logs/knative-crds.txt
        kubectl get crds | grep cert-manager > logs/cert-manager-crds.txt
        kubectl get crds | grep kserve > logs/kserve-crds.txt
        
        # Collect pod descriptions
        kubectl describe pods -n kubeflow > logs/kubeflow-pod-descriptions.txt
        kubectl describe pods -n istio-system > logs/istio-pod-descriptions.txt
        kubectl describe pods -n cert-manager > logs/cert-manager-pod-descriptions.txt
        kubectl describe pods -n auth > logs/auth-pod-descriptions.txt
        
        # Collect pod logs for all components referenced in kustomization.yaml
        kubectl logs -n kubeflow -l app=ml-pipeline --tail=200 > logs/pipeline-logs.txt || true
        kubectl logs -n kubeflow -l app=centraldashboard --tail=200 > logs/dashboard-logs.txt || true
        kubectl logs -n kubeflow -l app=profiles --tail=200 > logs/profiles-logs.txt || true
        kubectl logs -n kubeflow -l app=katib --tail=200 > logs/katib-logs.txt || true
        kubectl logs -n kubeflow -l app=jupyter-web-app --tail=200 > logs/jupyter-web-app-logs.txt || true
        kubectl logs -n kubeflow -l app=notebook-controller --tail=200 > logs/notebook-controller-logs.txt || true
        kubectl logs -n kubeflow -l app=tensorboard-controller --tail=200 > logs/tensorboard-controller-logs.txt || true
        kubectl logs -n kubeflow -l app=training-operator --tail=200 > logs/training-operator-logs.txt || true
        kubectl logs -n kubeflow -l app=kserve --tail=200 > logs/kserve-logs.txt || true
        kubectl logs -n auth -l app=oauth2-proxy --tail=200 > logs/oauth2-proxy-logs.txt || true
        kubectl logs -n auth -l app=dex --tail=200 > logs/dex-logs.txt || true
        
        # Collect specific component logs
        for pod in $(kubectl get pods -n kubeflow -o jsonpath='{.items[*].metadata.name}'); do
          kubectl logs -n kubeflow $pod --tail=100 > logs/kubeflow-$pod.txt 2>&1 || true
        done
        
        echo "Collected logs to logs/ directory"
        
    - name: Upload Logs
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: kubeflow-test-logs
        path: logs/ 