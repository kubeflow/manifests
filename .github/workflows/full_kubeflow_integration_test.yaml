name: Full Kubeflow Integration Test

on:
  workflow_dispatch:  # Allow manual triggering
  push:
    branches:
      - master

jobs:
  build:
    name: Full E2E Integration Test
    runs-on:
      labels: ubuntu-latest-16-cores
    env:
      KIND_CLUSTER_NAME: kubeflow

    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: Install Python dependencies
      run: |
        pip install pytest kubernetes kfp==2.11.0 kserve pytest-timeout pyyaml requests

    - name: Install KinD, Create KinD cluster and Install kustomize
      run: ./tests/gh-actions/install_KinD_create_KinD_cluster_install_kustomize.sh

    - name: Install kubectl
      run: ./tests/gh-actions/install_kubectl.sh

    - name: Create kubeflow namespace
      run: kustomize build common/kubeflow-namespace/base | kubectl apply -f -

    - name: Install Cert Manager
      run: ./tests/gh-actions/install_cert_manager.sh

    - name: Install Istio
      run: ./tests/gh-actions/install_istio.sh

    - name: Install oauth2-proxy
      run: ./tests/gh-actions/install_oauth2-proxy.sh

    - name: Deploy Full Kubeflow Stack
      run: |
        # Apply with retry logic using exact command from README
        echo "Installing full Kubeflow stack from example/kustomization.yaml"
        while ! kustomize build example | kubectl apply --server-side --force-conflicts -f -; do 
          echo "Retrying to apply resources"; 
          sleep 20; 
        done
        
        # Check deployment status
        kubectl get deployments --all-namespaces
        kubectl get pods --all-namespaces

    - name: Install kubeflow-istio-resources
      run: kustomize build common/istio-1-24/kubeflow-istio-resources/base | kubectl apply -f -

    - name: Wait for Critical Services
      run: |
        echo "Waiting for all deployments to be ready..."
        kubectl wait --for=condition=available --timeout=1200s deployment --all -n cert-manager || true
        kubectl wait --for=condition=available --timeout=1200s deployment --all -n istio-system || true
        kubectl wait --for=condition=available --timeout=1200s deployment --all -n auth || true
        kubectl wait --for=condition=available --timeout=1800s deployment --all -n kubeflow || true
        
        # Wait for specific critical components to be ready
        kubectl -n kubeflow wait --for=condition=Ready pods -l kustomize.component=profiles --timeout 180s || true
        kubectl -n kubeflow wait --for=condition=Ready pods -l app=centraldashboard --timeout 180s || true
        kubectl -n kubeflow wait --for=condition=Ready pods -l app=ml-pipeline --timeout 180s || true
        kubectl -n istio-system wait --for=condition=Ready pods --all --timeout 180s || true
        kubectl -n cert-manager wait --for=condition=Ready pods --all --timeout 180s || true
        kubectl -n auth wait --for=condition=Ready pods --all --timeout 180s || true

    - name: Create KF Profile
      run: |
        # Create user namespace
        kustomize build common/user-namespace/base | kubectl apply -f -
        sleep 60 # wait for profile controller to create resources
        
        # Ensure controllers are running
        METACONTROLLER_POD=$(kubectl get pods -n kubeflow -o json | jq -r '.items[] | select(.metadata.name | startswith("metacontroller")) | .metadata.name')
        if [[ -z "$METACONTROLLER_POD" ]]; then
          echo "Error: metacontroller pod not found in kubeflow namespace."
          exit 1
        fi
        kubectl logs -n kubeflow "$METACONTROLLER_POD"
        
        PIPELINES_PROFILE_CONTROLLER_POD=$(kubectl get pods -n kubeflow -o json | jq -r '.items[] | select(.metadata.name | startswith("kubeflow-pipelines-profile-controller")) | .metadata.name')
        if [[ -z "$PIPELINES_PROFILE_CONTROLLER_POD" ]]; then
          echo "Error: kubeflow-pipelines-profile-controller pod not found in kubeflow namespace."
          exit 1
        fi
        kubectl logs -n kubeflow "$PIPELINES_PROFILE_CONTROLLER_POD"
        
        KF_PROFILE=kubeflow-user-example-com
        kubectl -n $KF_PROFILE get pods,configmaps,secrets
        
        if ! kubectl get secret mlpipeline-minio-artifact -n $KF_PROFILE > /dev/null 2>&1; then
          echo "Error: Secret mlpipeline-minio-artifact not found in namespace $KF_PROFILE"
          exit 1
        fi
        kubectl get secret mlpipeline-minio-artifact -n "$KF_PROFILE" -o json | jq -r '.data | keys[] as $k | "\($k): \(. | .[$k] | @base64d)"' | tr '\n' ' '

    - name: Port Forward Istio Gateway
      run: |
        ingress_gateway_service=$(kubectl get svc --namespace istio-system --selector="app=istio-ingressgateway" --output jsonpath='{.items[0].metadata.name}')
        nohup kubectl port-forward --namespace istio-system svc/${ingress_gateway_service} 8080:80 &
        while ! curl localhost:8080; do echo waiting for port-forwarding; sleep 1; done; echo port-forwarding ready

    - name: Test Auth Integration
      if: always()
      continue-on-error: true
      run: |
        # Test if Dex login test script exists and run it
        if [ -f "tests/gh-actions/test_dex_login.py" ]; then
          echo "Running Dex login test..."
          python3 tests/gh-actions/test_dex_login.py
        else
          # Fallback to simple auth testing
          echo "Testing authentication redirect..."
          HTTP_STATUS=$(curl -s -o /dev/null -w "%{http_code}" http://localhost:8080/pipeline/)
          if [[ $HTTP_STATUS -eq 302 || $HTTP_STATUS -eq 401 ]]; then
            echo "Auth check successful: Got redirect or unauthorized as expected"
          else
            echo "Warning: Expected auth redirect, got HTTP status $HTTP_STATUS"
          fi
        fi

    - name: Run Pipeline Tests
      run: |
        KF_PROFILE=kubeflow-user-example-com
        
        # Test with authorized token
        TOKEN="$(kubectl -n $KF_PROFILE create token default-editor)"
        echo "Running pipeline with authorized token"
        python3 tests/gh-actions/pipeline_test.py run_pipeline "${TOKEN}" "${KF_PROFILE}"
        
        # Test with unauthorized token
        echo "Testing unauthorized access"
        TOKEN="$(kubectl -n default create token default)"
        python3 tests/gh-actions/pipeline_test.py test_unauthorized_access "${TOKEN}" "${KF_PROFILE}"
        echo "Test succeeded. Token from unauthorized ServiceAccount cannot list pipelines in $KF_PROFILE namespace."

    - name: Test Central Dashboard Access
      run: |
        # Test central dashboard is accessible
        curl -I http://localhost:8080/
        
        # Check other components
        echo "Checking Notebooks access"
        curl -I http://localhost:8080/jupyter/
        
        echo "Checking Pipelines access"
        curl -I http://localhost:8080/pipeline/
        
        echo "Checking KServe access"
        curl -I http://localhost:8080/models/
        
        echo "Checking Katib access"
        curl -I http://localhost:8080/katib/

    - name: Create Notebook Test
      if: always()
      run: |
        KF_PROFILE=kubeflow-user-example-com
        sed "s/kubeflow-user-example-com/$KF_PROFILE/g" tests/gh-actions/kf-objects/notebook.test.kubeflow-user-example.com.yaml > notebook-test.yaml
        
        # Apply the notebook
        kubectl apply -f notebook-test.yaml
        
        # Wait for notebook to be ready
        kubectl wait --for=condition=ready --timeout=300s -n ${KF_PROFILE} pod -l app=test || true
        
        # Verify notebook status
        kubectl get notebooks -n ${KF_PROFILE}

    - name: Test Katib Experiment
      if: always()
      continue-on-error: true
      run: |
        # Test if Katib is available
        if kubectl get crd experiments.kubeflow.org > /dev/null 2>&1; thents
          KF_PROFILE=kubeflow-user-example-com
          sed "s/kubeflow-user/$KF_PROFILE/g" tests/gh-actions/kf-objects/katib_test.yaml > katib-experiment.yaml
          
          kubectl apply -f katib-experiment.yaml || echo "Katib experiment creation failed, continuing tests"
          
          # Check if experiment was created
          kubectl get experiments -n ${KF_PROFILE} || echo "No Katib experiments found, continuing tests"
        else
          echo "Katib CRD not found, skipping Katib tests"
        fi

    - name: Test Training Operator
      if: always()
      continue-on-error: true
      run: |
        # First install Training Operator if needed
        if ! kubectl get crd tfjobs.kubeflow.org > /dev/null 2>&1; then
          ./tests/gh-actions/install_training_operator.sh || true
        fi
        
        # Test if Training Operator is available
        if kubectl get crd pytorchjobs.kubeflow.org > /dev/null 2>&1; then
          # Use the existing PyTorch job YAML from kf-objects
          KF_PROFILE=kubeflow-user-example-com
          cp tests/gh-actions/kf-objects/training_operator_job.yaml pytorch-job.yaml
          # Update namespace in the YAML
          sed -i "s/namespace: .*/namespace: $KF_PROFILE/g" pytorch-job.yaml || true
          
          kubectl apply -f pytorch-job.yaml || echo "PyTorch job creation failed, continuing tests"
          
          # Check if job was created
          kubectl get pytorchjobs -n ${KF_PROFILE} || echo "No PyTorch jobs found, continuing tests"
        else
          echo "Training Operator CRDs not found, skipping Training Operator tests"
        fi

    - name: Test KServe Model Deployment
      if: always()
      continue-on-error: true
      run: |
        # Install KServe if needed
        if ! kubectl get crd inferenceservices.serving.kserve.io > /dev/null 2>&1; then
          ./tests/gh-actions/install_kserve.sh || true
        fi
        
        # Only run if KServe is installed
        if kubectl get crd inferenceservices.serving.kserve.io > /dev/null 2>&1; then
          # Use the existing KServe inference service YAML from kf-objects
          KF_PROFILE=kubeflow-user-example-com
          cp tests/gh-actions/kf-objects/kserve_test.yaml kserve-test.yaml
          # Add namespace to the YAML
          sed -i "/metadata:/a\  namespace: $KF_PROFILE" kserve-test.yaml || true
          
          kubectl apply -f kserve-test.yaml || echo "KServe deployment failed, continuing tests"
          
          # Wait for InferenceService to be ready - timeout after 2min
          kubectl wait --for=condition=ready --timeout=120s -n ${KF_PROFILE} isvc/sklearn-iris || echo "InferenceService did not become ready in time, continuing tests"
        else
          echo "KServe CRD not found, skipping KServe tests"
        fi

    - name: Test Pod Security Standards
      if: always()
      continue-on-error: true
      run: |
        # Apply baseline PSS
        ./tests/gh-actions/enable_baseline_PSS.sh || true
        
        # Verify all pods still work
        kubectl get pods --all-namespaces
        
        # Unapply baseline labels
        echo "Unapplying baseline PSS labels..."
        NAMESPACES=("istio-system" "auth" "cert-manager" "oauth2-proxy" "kubeflow")
        for NAMESPACE in "${NAMESPACES[@]}"; do
          if kubectl get namespace "$NAMESPACE" >/dev/null 2>&1; then
            kubectl label namespace $NAMESPACE pod-security.kubernetes.io/enforce- || true
          fi
        done
        
        # Apply restricted PSS
        ./tests/gh-actions/enable_restricted_PSS.sh || true
        
        # Verify all pods still work
        kubectl get pods --all-namespaces

    - name: Verify All Components Running
      run: |
        # Check critical components are running
        kubectl get pods -n kubeflow
        
        # Check specific components from kustomization.yaml
        echo "Checking Cert-Manager status..."
        kubectl get pods -n cert-manager
        
        echo "Checking Istio status..."
        kubectl get pods -n istio-system
        
        echo "Checking Auth status (oauth2-proxy, dex)..."
        kubectl get pods -n auth
        
        echo "Checking KNative status..."
        kubectl get pods -n knative-serving || echo "KNative not deployed with dedicated namespace"
        
        echo "Checking Kubeflow components..."
        kubectl get deployment -n kubeflow centraldashboard || echo "Central Dashboard not found"
        kubectl get deployment -n kubeflow katib-controller || echo "Katib controller not found"
        kubectl get deployment -n kubeflow ml-pipeline || echo "Pipeline not found"
        kubectl get deployment -n kubeflow notebook-controller-deployment || echo "Notebook controller not found"
        kubectl get deployment -n kubeflow profiles-deployment || echo "Profiles not found"
        kubectl get deployment -n kubeflow tensorboard-controller-controller-manager || echo "Tensorboard controller not found"
        kubectl get deployment -n kubeflow training-operator || echo "Training operator not found"
        kubectl get deployment -n kubeflow kserve-controller-manager || echo "KServe not found"
        
        # Verify no pods are in failed state
        if kubectl get pods --all-namespaces | grep -E '(Error|CrashLoopBackOff)'; then
          echo "Found pods in failed state"
          exit 1
        fi
        
        echo "All Kubeflow components are running successfully"

    - name: Collect Logs on Failure
      if: failure()
      run: |
        mkdir -p logs
        
        # Collect resource status
        kubectl get all --all-namespaces > logs/all-resources.txt
        
        # Collect events
        kubectl get events --all-namespaces --sort-by=.metadata.creationTimestamp > logs/all-events.txt
        
        # Collect CRD status for all Kubeflow components
        echo "Collecting CRD status..."
        kubectl get crds | grep kubeflow > logs/kubeflow-crds.txt || true
        kubectl get crds | grep istio > logs/istio-crds.txt || true
        kubectl get crds | grep knative > logs/knative-crds.txt || true
        kubectl get crds | grep cert-manager > logs/cert-manager-crds.txt || true
        kubectl get crds | grep kserve > logs/kserve-crds.txt || true
        
        # Collect pod descriptions
        kubectl describe pods -n kubeflow > logs/kubeflow-pod-descriptions.txt
        kubectl describe pods -n istio-system > logs/istio-pod-descriptions.txt
        kubectl describe pods -n cert-manager > logs/cert-manager-pod-descriptions.txt
        kubectl describe pods -n auth > logs/auth-pod-descriptions.txt
        
        # Collect pod logs for all components referenced in kustomization.yaml
        kubectl logs -n kubeflow -l app=ml-pipeline --tail=200 > logs/pipeline-logs.txt || true
        kubectl logs -n kubeflow -l app=centraldashboard --tail=200 > logs/dashboard-logs.txt || true
        kubectl logs -n kubeflow -l app=profiles --tail=200 > logs/profiles-logs.txt || true
        kubectl logs -n kubeflow -l app=katib --tail=200 > logs/katib-logs.txt || true
        kubectl logs -n kubeflow -l app=jupyter-web-app --tail=200 > logs/jupyter-web-app-logs.txt || true
        kubectl logs -n kubeflow -l app=notebook-controller --tail=200 > logs/notebook-controller-logs.txt || true
        kubectl logs -n kubeflow -l app=tensorboard-controller --tail=200 > logs/tensorboard-controller-logs.txt || true
        kubectl logs -n kubeflow -l app=training-operator --tail=200 > logs/training-operator-logs.txt || true
        kubectl logs -n kubeflow -l app=kserve --tail=200 > logs/kserve-logs.txt || true
        kubectl logs -n auth -l app=oauth2-proxy --tail=200 > logs/oauth2-proxy-logs.txt || true
        kubectl logs -n auth -l app=dex --tail=200 > logs/dex-logs.txt || true
        
        # Collect specific component logs
        for pod in $(kubectl get pods -n kubeflow -o jsonpath='{.items[*].metadata.name}'); do
          kubectl logs -n kubeflow $pod --tail=100 > logs/kubeflow-$pod.txt 2>&1 || true
        done
        
        echo "Collected logs to logs/ directory"
        
    - name: Upload Logs
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: kubeflow-test-logs
        path: logs/ 