apiVersion: v1
kind: Namespace
metadata:
  labels:
    istio-injection: enabled
    serving.knative.dev/release: v0.22.1
  name: knative-serving
---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  labels:
    knative.dev/crd-install: "true"
    serving.knative.dev/release: v0.22.1
  name: certificates.networking.internal.knative.dev
spec:
  group: networking.internal.knative.dev
  names:
    categories:
    - knative-internal
    - networking
    kind: Certificate
    plural: certificates
    shortNames:
    - kcert
    singular: certificate
  scope: Namespaced
  versions:
  - additionalPrinterColumns:
    - jsonPath: .status.conditions[?(@.type=="Ready")].status
      name: Ready
      type: string
    - jsonPath: .status.conditions[?(@.type=="Ready")].reason
      name: Reason
      type: string
    name: v1alpha1
    schema:
      openAPIV3Schema:
        type: object
        x-kubernetes-preserve-unknown-fields: true
    served: true
    storage: true
    subresources:
      status: {}
---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  labels:
    duck.knative.dev/podspecable: "true"
    knative.dev/crd-install: "true"
    serving.knative.dev/release: v0.22.1
  name: configurations.serving.knative.dev
spec:
  group: serving.knative.dev
  names:
    categories:
    - all
    - knative
    - serving
    kind: Configuration
    plural: configurations
    shortNames:
    - config
    - cfg
    singular: configuration
  scope: Namespaced
  versions:
  - additionalPrinterColumns:
    - jsonPath: .status.latestCreatedRevisionName
      name: LatestCreated
      type: string
    - jsonPath: .status.latestReadyRevisionName
      name: LatestReady
      type: string
    - jsonPath: .status.conditions[?(@.type=='Ready')].status
      name: Ready
      type: string
    - jsonPath: .status.conditions[?(@.type=='Ready')].reason
      name: Reason
      type: string
    name: v1
    schema:
      openAPIV3Schema:
        type: object
        x-kubernetes-preserve-unknown-fields: true
    served: true
    storage: true
    subresources:
      status: {}
---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  labels:
    knative.dev/crd-install: "true"
  name: images.caching.internal.knative.dev
spec:
  group: caching.internal.knative.dev
  names:
    categories:
    - knative-internal
    - caching
    kind: Image
    plural: images
    shortNames:
    - img
    singular: image
  scope: Namespaced
  versions:
  - additionalPrinterColumns:
    - jsonPath: .spec.image
      name: Image
      type: string
    name: v1alpha1
    schema:
      openAPIV3Schema:
        type: object
        x-kubernetes-preserve-unknown-fields: true
    served: true
    storage: true
    subresources:
      status: {}
---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  labels:
    knative.dev/crd-install: "true"
    serving.knative.dev/release: v0.22.1
  name: ingresses.networking.internal.knative.dev
spec:
  group: networking.internal.knative.dev
  names:
    categories:
    - knative-internal
    - networking
    kind: Ingress
    plural: ingresses
    shortNames:
    - kingress
    - king
    singular: ingress
  scope: Namespaced
  versions:
  - additionalPrinterColumns:
    - jsonPath: .status.conditions[?(@.type=='Ready')].status
      name: Ready
      type: string
    - jsonPath: .status.conditions[?(@.type=='Ready')].reason
      name: Reason
      type: string
    name: v1alpha1
    schema:
      openAPIV3Schema:
        type: object
        x-kubernetes-preserve-unknown-fields: true
    served: true
    storage: true
    subresources:
      status: {}
---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  labels:
    knative.dev/crd-install: "true"
    serving.knative.dev/release: v0.22.1
  name: metrics.autoscaling.internal.knative.dev
spec:
  group: autoscaling.internal.knative.dev
  names:
    categories:
    - knative-internal
    - autoscaling
    kind: Metric
    plural: metrics
    singular: metric
  scope: Namespaced
  versions:
  - additionalPrinterColumns:
    - jsonPath: .status.conditions[?(@.type=='Ready')].status
      name: Ready
      type: string
    - jsonPath: .status.conditions[?(@.type=='Ready')].reason
      name: Reason
      type: string
    name: v1alpha1
    schema:
      openAPIV3Schema:
        type: object
        x-kubernetes-preserve-unknown-fields: true
    served: true
    storage: true
    subresources:
      status: {}
---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  labels:
    knative.dev/crd-install: "true"
    serving.knative.dev/release: v0.22.1
  name: podautoscalers.autoscaling.internal.knative.dev
spec:
  group: autoscaling.internal.knative.dev
  names:
    categories:
    - knative-internal
    - autoscaling
    kind: PodAutoscaler
    plural: podautoscalers
    shortNames:
    - kpa
    - pa
    singular: podautoscaler
  scope: Namespaced
  versions:
  - additionalPrinterColumns:
    - jsonPath: .status.desiredScale
      name: DesiredScale
      type: integer
    - jsonPath: .status.actualScale
      name: ActualScale
      type: integer
    - jsonPath: .status.conditions[?(@.type=='Ready')].status
      name: Ready
      type: string
    - jsonPath: .status.conditions[?(@.type=='Ready')].reason
      name: Reason
      type: string
    name: v1alpha1
    schema:
      openAPIV3Schema:
        type: object
        x-kubernetes-preserve-unknown-fields: true
    served: true
    storage: true
    subresources:
      status: {}
---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  labels:
    knative.dev/crd-install: "true"
    serving.knative.dev/release: v0.22.1
  name: revisions.serving.knative.dev
spec:
  group: serving.knative.dev
  names:
    categories:
    - all
    - knative
    - serving
    kind: Revision
    plural: revisions
    shortNames:
    - rev
    singular: revision
  scope: Namespaced
  versions:
  - additionalPrinterColumns:
    - jsonPath: .metadata.labels['serving\.knative\.dev/configuration']
      name: Config Name
      type: string
    - jsonPath: .status.serviceName
      name: K8s Service Name
      type: string
    - jsonPath: .metadata.labels['serving\.knative\.dev/configurationGeneration']
      name: Generation
      type: string
    - jsonPath: .status.conditions[?(@.type=='Ready')].status
      name: Ready
      type: string
    - jsonPath: .status.conditions[?(@.type=='Ready')].reason
      name: Reason
      type: string
    name: v1
    schema:
      openAPIV3Schema:
        type: object
        x-kubernetes-preserve-unknown-fields: true
    served: true
    storage: true
    subresources:
      status: {}
---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  labels:
    duck.knative.dev/addressable: "true"
    knative.dev/crd-install: "true"
    serving.knative.dev/release: v0.22.1
  name: routes.serving.knative.dev
spec:
  group: serving.knative.dev
  names:
    categories:
    - all
    - knative
    - serving
    kind: Route
    plural: routes
    shortNames:
    - rt
    singular: route
  scope: Namespaced
  versions:
  - additionalPrinterColumns:
    - jsonPath: .status.url
      name: URL
      type: string
    - jsonPath: .status.conditions[?(@.type=='Ready')].status
      name: Ready
      type: string
    - jsonPath: .status.conditions[?(@.type=='Ready')].reason
      name: Reason
      type: string
    name: v1
    schema:
      openAPIV3Schema:
        type: object
        x-kubernetes-preserve-unknown-fields: true
    served: true
    storage: true
    subresources:
      status: {}
---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  labels:
    knative.dev/crd-install: "true"
    serving.knative.dev/release: v0.22.1
  name: serverlessservices.networking.internal.knative.dev
spec:
  group: networking.internal.knative.dev
  names:
    categories:
    - knative-internal
    - networking
    kind: ServerlessService
    plural: serverlessservices
    shortNames:
    - sks
    singular: serverlessservice
  scope: Namespaced
  versions:
  - additionalPrinterColumns:
    - jsonPath: .spec.mode
      name: Mode
      type: string
    - jsonPath: .spec.numActivators
      name: Activators
      type: integer
    - jsonPath: .status.serviceName
      name: ServiceName
      type: string
    - jsonPath: .status.privateServiceName
      name: PrivateServiceName
      type: string
    - jsonPath: .status.conditions[?(@.type=='Ready')].status
      name: Ready
      type: string
    - jsonPath: .status.conditions[?(@.type=='Ready')].reason
      name: Reason
      type: string
    name: v1alpha1
    schema:
      openAPIV3Schema:
        type: object
        x-kubernetes-preserve-unknown-fields: true
    served: true
    storage: true
    subresources:
      status: {}
---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  labels:
    duck.knative.dev/addressable: "true"
    duck.knative.dev/podspecable: "true"
    knative.dev/crd-install: "true"
    serving.knative.dev/release: v0.22.1
  name: services.serving.knative.dev
spec:
  group: serving.knative.dev
  names:
    categories:
    - all
    - knative
    - serving
    kind: Service
    plural: services
    shortNames:
    - kservice
    - ksvc
    singular: service
  scope: Namespaced
  versions:
  - additionalPrinterColumns:
    - jsonPath: .status.url
      name: URL
      type: string
    - jsonPath: .status.latestCreatedRevisionName
      name: LatestCreated
      type: string
    - jsonPath: .status.latestReadyRevisionName
      name: LatestReady
      type: string
    - jsonPath: .status.conditions[?(@.type=='Ready')].status
      name: Ready
      type: string
    - jsonPath: .status.conditions[?(@.type=='Ready')].reason
      name: Reason
      type: string
    name: v1
    schema:
      openAPIV3Schema:
        properties:
          spec:
            properties:
              template:
                description: |
                  A template for the current desired application state.
                  Changes to `template` will cause a new Revision to be created as
                  defined in the lifecycle section. The contents of the Service's
                  RevisionTemplateSpec is used to create a corresponding Configuration.
                type: object
                x-kubernetes-preserve-unknown-fields: true
              traffic:
                description: |
                  Traffic specifies how to distribute traffic over a
                  collection of Revisions belonging to the Service. If traffic is
                  empty or not provided, defaults to 100% traffic to the latest
                  `Ready` Revision. The contents of the Service's TrafficTarget is
                  used to create a corresponding Route.
                items:
                  properties:
                    configurationName:
                      description: |
                        ConfigurationName of a configuration to whose latest revision we will send
                        this portion of traffic. When the "status.latestReadyRevisionName" of the
                        referenced configuration changes, we will automatically migrate traffic
                        from the prior "latest ready" revision to the new one.  This field is never
                        set in Route's status, only its spec.
                        This is mutually exclusive with RevisionName.
                      type: string
                    latestRevision:
                      description: |
                        `latestRevision` may be optionally provided to indicate
                        that the latest ready Revision of the Configuration should be used
                        for this traffic target. When provided latestRevision MUST be true
                        if revisionName is empty, and it MUST be false when revisionName is non-empty.
                      type: boolean
                    percent:
                      description: |
                        The percentage of requests which should be allocated
                        from the main Route domain name to the specified `revisionName` or
                        `configurationName`.
                        All `percent` values in `traffic` MUST sum to 100.
                      maximum: 100
                      minimum: 0
                      type: integer
                    revisionName:
                      description: |
                        A specific revision to which to send this portion
                        of traffic.
                        This is mutually exclusive with configurationName.
                      type: string
                    tag:
                      description: |
                        Tag is optionally used to expose a dedicated URL for
                        referencing this target exclusively. The dedicated URL MUST include
                        in it the string provided by tag.
                      type: string
                  type: object
                  x-kubernetes-preserve-unknown-fields: true
                type: array
                x-kubernetes-preserve-unknown-fields: true
            type: object
            x-kubernetes-preserve-unknown-fields: true
          status:
            type: object
            x-kubernetes-preserve-unknown-fields: true
        type: object
        x-kubernetes-preserve-unknown-fields: true
    served: true
    storage: true
    subresources:
      status: {}
---
apiVersion: admissionregistration.k8s.io/v1
kind: MutatingWebhookConfiguration
metadata:
  labels:
    networking.knative.dev/ingress-provider: istio
    serving.knative.dev/release: v0.22.1
  name: webhook.istio.networking.internal.knative.dev
webhooks:
- admissionReviewVersions:
  - v1beta1
  clientConfig:
    service:
      name: istio-webhook
      namespace: knative-serving
  failurePolicy: Fail
  name: webhook.istio.networking.internal.knative.dev
  objectSelector:
    matchExpressions:
    - key: serving.knative.dev/configuration
      operator: Exists
  sideEffects: None
---
apiVersion: admissionregistration.k8s.io/v1
kind: MutatingWebhookConfiguration
metadata:
  labels:
    serving.knative.dev/release: v0.22.1
  name: webhook.serving.knative.dev
webhooks:
- admissionReviewVersions:
  - v1
  - v1beta1
  clientConfig:
    service:
      name: webhook
      namespace: knative-serving
  failurePolicy: Fail
  name: webhook.serving.knative.dev
  sideEffects: None
  timeoutSeconds: 10
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    serving.knative.dev/release: v0.22.1
  name: controller
  namespace: knative-serving
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    duck.knative.dev/addressable: "true"
    serving.knative.dev/release: v0.22.1
  name: knative-serving-addressable-resolver
rules:
- apiGroups:
  - serving.knative.dev
  resources:
  - routes
  - routes/status
  - services
  - services/status
  verbs:
  - get
  - list
  - watch
---
aggregationRule:
  clusterRoleSelectors:
  - matchLabels:
      serving.knative.dev/controller: "true"
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    serving.knative.dev/release: v0.22.1
  name: knative-serving-admin
rules: []
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    serving.knative.dev/controller: "true"
    serving.knative.dev/release: v0.22.1
  name: knative-serving-core
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - namespaces
  - secrets
  - configmaps
  - endpoints
  - services
  - events
  - serviceaccounts
  verbs:
  - get
  - list
  - create
  - update
  - delete
  - patch
  - watch
- apiGroups:
  - ""
  resources:
  - endpoints/restricted
  verbs:
  - create
- apiGroups:
  - apps
  resources:
  - deployments
  - deployments/finalizers
  verbs:
  - get
  - list
  - create
  - update
  - delete
  - patch
  - watch
- apiGroups:
  - admissionregistration.k8s.io
  resources:
  - mutatingwebhookconfigurations
  - validatingwebhookconfigurations
  verbs:
  - get
  - list
  - create
  - update
  - delete
  - patch
  - watch
- apiGroups:
  - apiextensions.k8s.io
  resources:
  - customresourcedefinitions
  - customresourcedefinitions/status
  verbs:
  - get
  - list
  - create
  - update
  - delete
  - patch
  - watch
- apiGroups:
  - autoscaling
  resources:
  - horizontalpodautoscalers
  verbs:
  - get
  - list
  - create
  - update
  - delete
  - patch
  - watch
- apiGroups:
  - coordination.k8s.io
  resources:
  - leases
  verbs:
  - get
  - list
  - create
  - update
  - delete
  - patch
  - watch
- apiGroups:
  - serving.knative.dev
  - autoscaling.internal.knative.dev
  - networking.internal.knative.dev
  resources:
  - '*'
  - '*/status'
  - '*/finalizers'
  verbs:
  - get
  - list
  - create
  - update
  - delete
  - deletecollection
  - patch
  - watch
- apiGroups:
  - caching.internal.knative.dev
  resources:
  - images
  verbs:
  - get
  - list
  - create
  - update
  - delete
  - patch
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    networking.knative.dev/ingress-provider: istio
    serving.knative.dev/controller: "true"
    serving.knative.dev/release: v0.22.1
  name: knative-serving-istio
rules:
- apiGroups:
  - networking.istio.io
  resources:
  - virtualservices
  - gateways
  - destinationrules
  verbs:
  - get
  - list
  - create
  - update
  - delete
  - patch
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    rbac.authorization.k8s.io/aggregate-to-admin: "true"
    rbac.authorization.kubeflow.org/aggregate-to-kubeflow-admin: "true"
    serving.knative.dev/release: v0.22.1
  name: knative-serving-namespaced-admin
rules:
- apiGroups:
  - serving.knative.dev
  resources:
  - '*'
  verbs:
  - '*'
- apiGroups:
  - networking.internal.knative.dev
  - autoscaling.internal.knative.dev
  - caching.internal.knative.dev
  resources:
  - '*'
  verbs:
  - get
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    rbac.authorization.k8s.io/aggregate-to-edit: "true"
    rbac.authorization.kubeflow.org/aggregate-to-kubeflow-edit: "true"
    serving.knative.dev/release: v0.22.1
  name: knative-serving-namespaced-edit
rules:
- apiGroups:
  - serving.knative.dev
  resources:
  - '*'
  verbs:
  - create
  - update
  - patch
  - delete
- apiGroups:
  - networking.internal.knative.dev
  - autoscaling.internal.knative.dev
  - caching.internal.knative.dev
  resources:
  - '*'
  verbs:
  - get
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    rbac.authorization.k8s.io/aggregate-to-view: "true"
    rbac.authorization.kubeflow.org/aggregate-to-kubeflow-view: "true"
    serving.knative.dev/release: v0.22.1
  name: knative-serving-namespaced-view
rules:
- apiGroups:
  - serving.knative.dev
  - networking.internal.knative.dev
  - autoscaling.internal.knative.dev
  - caching.internal.knative.dev
  resources:
  - '*'
  verbs:
  - get
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    duck.knative.dev/podspecable: "true"
    serving.knative.dev/release: v0.22.1
  name: knative-serving-podspecable-binding
rules:
- apiGroups:
  - serving.knative.dev
  resources:
  - configurations
  - services
  verbs:
  - list
  - watch
  - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    serving.knative.dev/release: v0.22.1
  name: knative-serving-controller-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: knative-serving-admin
subjects:
- kind: ServiceAccount
  name: controller
  namespace: knative-serving
---
apiVersion: v1
data:
  _example: |
    ################################
    #                              #
    #    EXAMPLE CONFIGURATION     #
    #                              #
    ################################

    # This block is not actually functional configuration,
    # but serves to illustrate the available configuration
    # options and document them in a way that is accessible
    # to users that `kubectl edit` this config map.
    #
    # These sample configuration options may be copied out of
    # this example block and unindented to be in the data block
    # to actually change the configuration.

    # The Revision ContainerConcurrency field specifies the maximum number
    # of requests the Container can handle at once. Container concurrency
    # target percentage is how much of that maximum to use in a stable
    # state. E.g. if a Revision specifies ContainerConcurrency of 10, then
    # the Autoscaler will try to maintain 7 concurrent connections per pod
    # on average.
    # Note: this limit will be applied to container concurrency set at every
    # level (ConfigMap, Revision Spec or Annotation).
    # For legacy and backwards compatibility reasons, this value also accepts
    # fractional values in (0, 1] interval (i.e. 0.7 â‡’ 70%).
    # Thus minimal percentage value must be greater than 1.0, or it will be
    # treated as a fraction.
    # NOTE: that this value does not affect actual number of concurrent requests
    #       the user container may receive, but only the average number of requests
    #       that the revision pods will receive.
    container-concurrency-target-percentage: "70"

    # The container concurrency target default is what the Autoscaler will
    # try to maintain when concurrency is used as the scaling metric for the
    # Revision and the Revision specifies unlimited concurrency.
    # When revision explicitly specifies container concurrency, that value
    # will be used as a scaling target for autoscaler.
    # When specifying unlimited concurrency, the autoscaler will
    # horizontally scale the application based on this target concurrency.
    # This is what we call "soft limit" in the documentation, i.e. it only
    # affects number of pods and does not affect the number of requests
    # individual pod processes.
    # The value must be a positive number such that the value multiplied
    # by container-concurrency-target-percentage is greater than 0.01.
    # NOTE: that this value will be adjusted by application of
    #       container-concurrency-target-percentage, i.e. by default
    #       the system will target on average 70 concurrent requests
    #       per revision pod.
    # NOTE: Only one metric can be used for autoscaling a Revision.
    container-concurrency-target-default: "100"

    # The requests per second (RPS) target default is what the Autoscaler will
    # try to maintain when RPS is used as the scaling metric for a Revision and
    # the Revision specifies unlimited RPS. Even when specifying unlimited RPS,
    # the autoscaler will horizontally scale the application based on this
    # target RPS.
    # Must be greater than 1.0.
    # NOTE: Only one metric can be used for autoscaling a Revision.
    requests-per-second-target-default: "200"

    # The target burst capacity specifies the size of burst in concurrent
    # requests that the system operator expects the system will receive.
    # Autoscaler will try to protect the system from queueing by introducing
    # Activator in the request path if the current spare capacity of the
    # service is less than this setting.
    # If this setting is 0, then Activator will be in the request path only
    # when the revision is scaled to 0.
    # If this setting is > 0 and container-concurrency-target-percentage is
    # 100% or 1.0, then activator will always be in the request path.
    # -1 denotes unlimited target-burst-capacity and activator will always
    # be in the request path.
    # Other negative values are invalid.
    target-burst-capacity: "200"

    # When operating in a stable mode, the autoscaler operates on the
    # average concurrency over the stable window.
    # Stable window must be in whole seconds.
    stable-window: "60s"

    # When observed average concurrency during the panic window reaches
    # panic-threshold-percentage the target concurrency, the autoscaler
    # enters panic mode. When operating in panic mode, the autoscaler
    # scales on the average concurrency over the panic window which is
    # panic-window-percentage of the stable-window.
    # Must be in the [1, 100] range.
    # When computing the panic window it will be rounded to the closest
    # whole second, at least 1s.
    panic-window-percentage: "10.0"

    # The percentage of the container concurrency target at which to
    # enter panic mode when reached within the panic window.
    panic-threshold-percentage: "200.0"

    # Max scale up rate limits the rate at which the autoscaler will
    # increase pod count. It is the maximum ratio of desired pods versus
    # observed pods.
    # Cannot be less or equal to 1.
    # I.e with value of 2.0 the number of pods can at most go N to 2N
    # over single Autoscaler period (2s), but at least N to
    # N+1, if Autoscaler needs to scale up.
    max-scale-up-rate: "1000.0"

    # Max scale down rate limits the rate at which the autoscaler will
    # decrease pod count. It is the maximum ratio of observed pods versus
    # desired pods.
    # Cannot be less or equal to 1.
    # I.e. with value of 2.0 the number of pods can at most go N to N/2
    # over single Autoscaler evaluation period (2s), but at
    # least N to N-1, if Autoscaler needs to scale down.
    max-scale-down-rate: "2.0"

    # Scale to zero feature flag.
    enable-scale-to-zero: "true"

    # Scale to zero grace period is the time an inactive revision is left
    # running before it is scaled to zero (must be positive, but recommended
    # at least a few seconds if running with mesh networking).
    # This is the upper limit and is provided not to enforce timeout after
    # the revision stopped receiving requests for stable window, but to
    # ensure network reprogramming to put activator in the path has completed.
    # If the system determines that a shorter period is satisfactory,
    # then the system will only wait that amount of time before scaling to 0.
    # NOTE: this period might actually be 0, if activator has been
    # in the request path sufficiently long.
    # If there is necessity for the last pod to linger longer use
    # scale-to-zero-pod-retention-period flag.
    scale-to-zero-grace-period: "30s"

    # Scale to zero pod retention period defines the minimum amount
    # of time the last pod will remain after Autoscaler has decided to
    # scale to zero.
    # This flag is for the situations where the pod startup is very expensive
    # and the traffic is bursty (requiring smaller windows for fast action),
    # but patchy.
    # The larger of this flag and `scale-to-zero-grace-period` will effectively
    # determine how the last pod will hang around.
    scale-to-zero-pod-retention-period: "0s"

    # pod-autoscaler-class specifies the default pod autoscaler class
    # that should be used if none is specified. If omitted, the Knative
    # Horizontal Pod Autoscaler (KPA) is used by default.
    pod-autoscaler-class: "kpa.autoscaling.knative.dev"

    # The capacity of a single activator task.
    # The `unit` is one concurrent request proxied by the activator.
    # activator-capacity must be at least 1.
    # This value is used for computation of the Activator subset size.
    # See the algorithm here: http://bit.ly/38XiCZ3.
    # TODO(vagababov): tune after actual benchmarking.
    activator-capacity: "100.0"

    # initial-scale is the cluster-wide default value for the initial target
    # scale of a revision after creation, unless overridden by the
    # "autoscaling.knative.dev/initialScale" annotation.
    # This value must be greater than 0 unless allow-zero-initial-scale is true.
    initial-scale: "1"

    # allow-zero-initial-scale controls whether either the cluster-wide initial-scale flag,
    # or the "autoscaling.knative.dev/initialScale" annotation, can be set to 0.
    allow-zero-initial-scale: "false"

    # max-scale is the cluster-wide default value for the max scale of a revision,
    # unless overridden by the "autoscaling.knative.dev/maxScale" annotation.
    # If set to 0, the revision has no maximum scale.
    max-scale: "0"

    # scale-down-delay is the amount of time that must pass at reduced
    # concurrency before a scale down decision is applied. This can be useful,
    # for example, to maintain replica count and avoid a cold start penalty if
    # more requests come in within the scale down delay period.
    # The default, 0s, imposes no delay at all.
    scale-down-delay: "0s"

    # max-scale-limit sets the maximum permitted value for the max scale of a revision.
    # When this is set to a positive value, a revision with a maxScale above that value
    # (including a maxScale of "0" = unlimited) is disallowed.
    # A value of zero (the default) allows any limit, including unlimited.
    max-scale-limit: "0"
kind: ConfigMap
metadata:
  annotations:
    knative.dev/example-checksum: 604cb513
  labels:
    serving.knative.dev/release: v0.22.1
  name: config-autoscaler
  namespace: knative-serving
---
apiVersion: v1
data:
  _example: |
    ################################
    #                              #
    #    EXAMPLE CONFIGURATION     #
    #                              #
    ################################

    # This block is not actually functional configuration,
    # but serves to illustrate the available configuration
    # options and document them in a way that is accessible
    # to users that `kubectl edit` this config map.
    #
    # These sample configuration options may be copied out of
    # this example block and unindented to be in the data block
    # to actually change the configuration.

    # revision-timeout-seconds contains the default number of
    # seconds to use for the revision's per-request timeout, if
    # none is specified.
    revision-timeout-seconds: "300"  # 5 minutes

    # max-revision-timeout-seconds contains the maximum number of
    # seconds that can be used for revision-timeout-seconds.
    # This value must be greater than or equal to revision-timeout-seconds.
    # If omitted, the system default is used (600 seconds).
    #
    # If this value is increased, the activator's terminationGraceTimeSeconds
    # should also be increased to prevent in-flight requests being disrupted.
    max-revision-timeout-seconds: "600"  # 10 minutes

    # revision-cpu-request contains the cpu allocation to assign
    # to revisions by default.  If omitted, no value is specified
    # and the system default is used.
    # Below is an example of setting revision-cpu-request.
    # By default, it is not set by Knative.
    revision-cpu-request: "400m"  # 0.4 of a CPU (aka 400 milli-CPU)

    # revision-memory-request contains the memory allocation to assign
    # to revisions by default.  If omitted, no value is specified
    # and the system default is used.
    # Below is an example of setting revision-memory-request.
    # By default, it is not set by Knative.
    revision-memory-request: "100M"  # 100 megabytes of memory

    # revision-ephemeral-storage-request contains the ephemeral storage
    # allocation to assign to revisions by default.  If omitted, no value is
    # specified and the system default is used.
    revision-ephemeral-storage-request: "500M"  # 500 megabytes of storage

    # revision-cpu-limit contains the cpu allocation to limit
    # revisions to by default.  If omitted, no value is specified
    # and the system default is used.
    # Below is an example of setting revision-cpu-limit.
    # By default, it is not set by Knative.
    revision-cpu-limit: "1000m"  # 1 CPU (aka 1000 milli-CPU)

    # revision-memory-limit contains the memory allocation to limit
    # revisions to by default.  If omitted, no value is specified
    # and the system default is used.
    # Below is an example of setting revision-memory-limit.
    # By default, it is not set by Knative.
    revision-memory-limit: "200M"  # 200 megabytes of memory

    # revision-ephemeral-storage-limit contains the ephemeral storage
    # allocation to limit revisions to by default.  If omitted, no value is
    # specified and the system default is used.
    revision-ephemeral-storage-limit: "750M"  # 750 megabytes of storage

    # container-name-template contains a template for the default
    # container name, if none is specified.  This field supports
    # Go templating and is supplied with the ObjectMeta of the
    # enclosing Service or Configuration, so values such as
    # {{.Name}} are also valid.
    container-name-template: "user-container"

    # container-concurrency specifies the maximum number
    # of requests the Container can handle at once, and requests
    # above this threshold are queued.  Setting a value of zero
    # disables this throttling and lets through as many requests as
    # the pod receives.
    container-concurrency: "0"

    # The container concurrency max limit is an operator setting ensuring that
    # the individual revisions cannot have arbitrary large concurrency
    # values, or autoscaling targets. `container-concurrency` default setting
    # must be at or below this value.
    #
    # Must be greater than 1.
    #
    # Note: even with this set, a user can choose a containerConcurrency
    # of 0 (i.e. unbounded) unless allow-container-concurrency-zero is
    # set to "false".
    container-concurrency-max-limit: "1000"

    # allow-container-concurrency-zero controls whether users can
    # specify 0 (i.e. unbounded) for containerConcurrency.
    allow-container-concurrency-zero: "true"

    # enable-service-links specifies the default value used for the
    # enableServiceLinks field of the PodSpec, when it is omitted by the user.
    # See: https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/#accessing-the-service
    #
    # This is a tri-state flag with possible values of (true|false|default).
    #
    # In environments with large number of services it is suggested
    # to set this value to `false`.
    # See https://github.com/knative/serving/issues/8498.
    enable-service-links: "false"
kind: ConfigMap
metadata:
  annotations:
    knative.dev/example-checksum: cdabec96
  labels:
    serving.knative.dev/release: v0.22.1
  name: config-defaults
  namespace: knative-serving
---
apiVersion: v1
data:
  _example: |
    ################################
    #                              #
    #    EXAMPLE CONFIGURATION     #
    #                              #
    ################################

    # This block is not actually functional configuration,
    # but serves to illustrate the available configuration
    # options and document them in a way that is accessible
    # to users that `kubectl edit` this config map.
    #
    # These sample configuration options may be copied out of
    # this example block and unindented to be in the data block
    # to actually change the configuration.

    # List of repositories for which tag to digest resolving should be skipped
    registriesSkippingTagResolving: "kind.local,ko.local,dev.local"

    # digestResolutionTimeout is the maximum time allowed for an image's
    # digests to be resolved.
    digestResolutionTimeout: "10s"

    # ProgressDeadline is the duration we wait for the deployment to
    # be ready before considering it failed.
    progressDeadline: "600s"

    # queueSidecarCPURequest is the requests.cpu to set for the queue proxy sidecar container.
    # If omitted, a default value (currently "25m"), is used.
    queueSidecarCPURequest: "25m"

    # queueSidecarCPULimit is the limits.cpu to set for the queue proxy sidecar container.
    # If omitted, no value is specified and the system default is used.
    queueSidecarCPULimit: "1000m"

    # queueSidecarMemoryRequest is the requests.memory to set for the queue proxy container.
    # If omitted, no value is specified and the system default is used.
    queueSidecarMemoryRequest: "400Mi"

    # queueSidecarMemoryLimit is the limits.memory to set for the queue proxy container.
    # If omitted, no value is specified and the system default is used.
    queueSidecarMemoryLimit: "800Mi"

    # queueSidecarEphemeralStorageRequest is the requests.ephemeral-storage to
    # set for the queue proxy sidecar container.
    # If omitted, no value is specified and the system default is used.
    queueSidecarEphemeralStorageRequest: "512Mi"

    # queueSidecarEphemeralStorageLimit is the limits.ephemeral-storage to set
    # for the queue proxy sidecar container.
    # If omitted, no value is specified and the system default is used.
    queueSidecarEphemeralStorageLimit: "1024Mi"
  progressDeadline: 600s
  queueSidecarImage: registry.cn-guangzhou.aliyuncs.com/k8s-docker-panyx/knative-dev_serving_cmd_queue:0b8e03117035
kind: ConfigMap
metadata:
  annotations:
    knative.dev/example-checksum: fa67b403
  labels:
    serving.knative.dev/release: v0.22.1
  name: config-deployment
  namespace: knative-serving
---
apiVersion: v1
data:
  _example: |
    ################################
    #                              #
    #    EXAMPLE CONFIGURATION     #
    #                              #
    ################################

    # This block is not actually functional configuration,
    # but serves to illustrate the available configuration
    # options and document them in a way that is accessible
    # to users that `kubectl edit` this config map.
    #
    # These sample configuration options may be copied out of
    # this example block and unindented to be in the data block
    # to actually change the configuration.

    # Default value for domain.
    # Although it will match all routes, it is the least-specific rule so it
    # will only be used if no other domain matches.
    example.com: |

    # These are example settings of domain.
    # example.org will be used for routes having app=nonprofit.
    example.org: |
      selector:
        app: nonprofit

    # Routes having the cluster domain suffix (by default 'svc.cluster.local')
    # will not be exposed through Ingress. You can define your own label
    # selector to assign that domain suffix to your Route here, or you can set
    # the label
    #    "serving.knative.dev/visibility=cluster-local"
    # to achieve the same effect.  This shows how to make routes having
    # the label app=secret only exposed to the local cluster.
    svc.cluster.local: |
      selector:
        app: secret
kind: ConfigMap
metadata:
  annotations:
    knative.dev/example-checksum: 74c3fc6a
  labels:
    serving.knative.dev/release: v0.22.1
  name: config-domain
  namespace: knative-serving
---
apiVersion: v1
data:
  _example: |-
    ################################
    #                              #
    #    EXAMPLE CONFIGURATION     #
    #                              #
    ################################

    # This block is not actually functional configuration,
    # but serves to illustrate the available configuration
    # options and document them in a way that is accessible
    # to users that `kubectl edit` this config map.
    #
    # These sample configuration options may be copied out of
    # this example block and unindented to be in the data block
    # to actually change the configuration.

    # Indicates whether multi container support is enabled
    #
    # WARNING: Cannot safely be disabled once enabled.
    # See: https://knative.dev/docs/serving/feature-flags/#multi-containers
    multi-container: "enabled"

    # Indicates whether Kubernetes affinity support is enabled
    #
    # WARNING: Cannot safely be disabled once enabled.
    # See: https://knative.dev/docs/serving/feature-flags/#kubernetes-node-affinity
    kubernetes.podspec-affinity: "disabled"

    # Indicates whether Kubernetes hostAliases support is enabled
    #
    # WARNING: Cannot safely be disabled once enabled.
    # See: https://knative.dev/docs/serving/feature-flags/#kubernetes-host-aliases
    kubernetes.podspec-hostaliases: "disabled"

    # Indicates whether Kubernetes nodeSelector support is enabled
    #
    # WARNING: Cannot safely be disabled once enabled.
    # See: https://knative.dev/docs/serving/feature-flags/#kubernetes-node-selector
    kubernetes.podspec-nodeselector: "disabled"

    # Indicates whether Kubernetes tolerations support is enabled
    #
    # WARNING: Cannot safely be disabled once enabled
    # See: https://knative.dev/docs/serving/feature-flags/#kubernetes-toleration
    kubernetes.podspec-tolerations: "disabled"

    # Indicates whether Kubernetes FieldRef support is enabled
    #
    # WARNING: Cannot safely be disabled once enabled.
    # See: https://knative.dev/docs/serving/feature-flags/#kubernetes-fieldref
    kubernetes.podspec-fieldref: "disabled"

    # Indicates whether Kubernetes RuntimeClassName support is enabled
    #
    # WARNING: Cannot safely be disabled once enabled.
    # See: https://knative.dev/docs/serving/feature-flags/#kubernetes-runtime-class
    kubernetes.podspec-runtimeclassname: "disabled"

    # This feature allows end-users to set a subset of fields on the Pod's SecurityContext
    # in addition to expanding the allowable fields within a Container's SecurityContext.
    #
    # When set to "enabled" or "allowed" it allows the following
    # PodSecurityContext properties:
    # - FSGroup
    # - RunAsGroup
    # - RunAsNonRoot
    # - SupplementalGroups
    # - RunAsUser
    #
    # When set to "enabled" or "allowed" it allows the following
    # Container SecurityContext properties:
    # - RunAsNonRoot
    # - RunAsGroup
    # - RunAsUser (already allowed without this flag)
    #
    # This feature flag should be used with caution as the PodSecurityContext
    # properties may have a side-effect on non-user sidecar containers that come
    # from Knative or your service mesh
    #
    # WARNING: Cannot safely be disabled once enabled.
    # See: https://knative.dev/docs/serving/feature-flags/#kubernetes-security-context
    kubernetes.podspec-securitycontext: "disabled"

    # This feature validates PodSpecs from the validating webhook
    # against the K8s API Server.
    #
    # When "enabled", the server will always run the extra validation.
    # When "allowed", the server will not run the dry-run validation by default.
    #   However, clients may enable the behavior on an individual Service by
    #   attaching the following metadata annotation: "features.knative.dev/podspec-dryrun":"enabled".
    # See: https://knative.dev/docs/serving/feature-flags/#kubernetes-dry-run
    kubernetes.podspec-dryrun: "allowed"

    # Indicates whether new responsive garbage collection is enabled. This
    # feature labels revisions in real-time as they become referenced and
    # dereferenced by Routes. This allows us to reap revisions shortly after
    # they are no longer active.
    # See: https://knative.dev/docs/serving/feature-flags/#responsive-revision-garbage-collector
    responsive-revision-gc: "enabled"

    # Controls whether tag header based routing feature are enabled or not.
    # 1. Enabled: enabling tag header based routing
    # 2. Disabled: disabling tag header based routing
    # See: https://knative.dev/docs/serving/feature-flags/#tag-header-based-routing
    tag-header-based-routing: "disabled"

    # Controls whether http2 auto-detection should be enabled or not.
    # 1. Enabled: http2 connection will be attempted via upgrade.
    # 2. Disabled: http2 connection will only be attempted when port name is set to "h2c".
    autodetect-http2: "disabled"
kind: ConfigMap
metadata:
  annotations:
    knative.dev/example-checksum: 2cf73688
  labels:
    serving.knative.dev/release: v0.22.1
  name: config-features
  namespace: knative-serving
---
apiVersion: v1
data:
  _example: |
    ################################
    #                              #
    #    EXAMPLE CONFIGURATION     #
    #                              #
    ################################

    # This block is not actually functional configuration,
    # but serves to illustrate the available configuration
    # options and document them in a way that is accessible
    # to users that `kubectl edit` this config map.
    #
    # These sample configuration options may be copied out of
    # this example block and unindented to be in the data block
    # to actually change the configuration.


    # ---------------------------------------
    # Garbage Collector Settings
    # ---------------------------------------
    #
    # Active
    #   * Revisions which are referenced by a Route are considered active.
    #   * Individual revisions may be marked with the annotation
    #      "knative.dev/no-gc":"true" to be permanently considered active.
    #   * Active revisions are not considered for GC.
    # Retention
    #   * Revisions are retained if they are any of the following:
    #       1. Active
    #       2. Were created within "retain-since-create-time"
    #       3. Were last referenced by a route within
    #           "retain-since-last-active-time"
    #       4. There are fewer than "min-non-active-revisions"
    #     If none of these conditions are met, or if the count of revisions exceed
    #      "max-non-active-revisions", they will be deleted by GC.
    #     The special value "disabled" may be used to turn off these limits.
    #
    # Example config to immediately collect any inactive revision:
    #    min-non-active-revisions: "0"
    #    retain-since-create-time: "disabled"
    #    retain-since-last-active-time: "disabled"
    #
    # Example config to always keep around the last ten non-active revisions:
    #     retain-since-create-time: "disabled"
    #     retain-since-last-active-time: "disabled"
    #     max-non-active-revisions: "10"
    #
    # Example config to disable all GC:
    #     retain-since-create-time: "disabled"
    #     retain-since-last-active-time: "disabled"
    #     max-non-active-revisions: "disabled"
    #
    # Example config to keep recently deployed or active revisions,
    # always maintain the last two in case of rollback, and prevent
    # burst activity from exploding the count of old revisions:
    #      retain-since-create-time: "48h"
    #      retain-since-last-active-time: "15h"
    #      min-non-active-revisions: "2"
    #      max-non-active-revisions: "1000"

    # Duration since creation before considering a revision for GC or "disabled".
    retain-since-create-time: "48h"

    # Duration since active before considering a revision for GC or "disabled".
    retain-since-last-active-time: "15h"

    # Minimum number of non-active revisions to retain.
    min-non-active-revisions: "20"

    # Maximum number of non-active revisions to retain
    # or "disabled" to disable any maximum limit.
    max-non-active-revisions: "1000"
kind: ConfigMap
metadata:
  annotations:
    knative.dev/example-checksum: e6149382
  labels:
    serving.knative.dev/release: v0.22.1
  name: config-gc
  namespace: knative-serving
---
apiVersion: v1
data:
  _example: |
    ################################
    #                              #
    #    EXAMPLE CONFIGURATION     #
    #                              #
    ################################

    # This block is not actually functional configuration,
    # but serves to illustrate the available configuration
    # options and document them in a way that is accessible
    # to users that `kubectl edit` this config map.
    #
    # These sample configuration options may be copied out of
    # this example block and unindented to be in the data block
    # to actually change the configuration.

    # Default Knative Gateway after v0.3. It points to the Istio
    # standard istio-ingressgateway, instead of a custom one that we
    # used pre-0.3. The configuration format should be `gateway.
    # {{gateway_namespace}}.{{gateway_name}}: "{{ingress_name}}.
    # {{ingress_namespace}}.svc.cluster.local"`. The {{gateway_namespace}}
    # is optional; when it is omitted, the system will search for
    # the gateway in the serving system namespace `knative-serving`
    gateway.knative-serving.knative-ingress-gateway: "istio-ingressgateway.istio-system.svc.cluster.local"

    # A cluster local gateway to allow pods outside of the mesh to access
    # Services and Routes not exposing through an ingress.  If the users
    # do have a service mesh setup, this isn't required and can be removed.
    #
    # An example use case is when users want to use Istio without any
    # sidecar injection (like Knative's istio-ci-no-mesh.yaml).  Since every pod
    # is outside of the service mesh in that case, a cluster-local  service
    # will need to be exposed to a cluster-local gateway to be accessible.
    # The configuration format should be `local-gateway.{{local_gateway_namespace}}.
    # {{local_gateway_name}}: "{{cluster_local_gateway_name}}.
    # {{cluster_local_gateway_namespace}}.svc.cluster.local"`. The
    # {{local_gateway_namespace}} is optional; when it is omitted, the system
    # will search for the local gateway in the serving system namespace
    # `knative-serving`
    local-gateway.knative-serving.knative-local-gateway: "knative-local-gateway.istio-system.svc.cluster.local"

    # To use only Istio service mesh and no knative-local-gateway, replace
    # all local-gateway.* entries by the following entry.
    local-gateway.mesh: "mesh"

    # If true, knative will use the Istio VirtualService's status to determine
    # endpoint readiness. Otherwise, probe as usual.
    enable-virtualservice-status: "false"
  gateway.kubeflow.kubeflow-gateway: istio-ingressgateway.istio-system.svc.cluster.local
kind: ConfigMap
metadata:
  labels:
    networking.knative.dev/ingress-provider: istio
    serving.knative.dev/release: v0.22.1
  name: config-istio
  namespace: knative-serving
---
apiVersion: v1
data:
  _example: |
    ################################
    #                              #
    #    EXAMPLE CONFIGURATION     #
    #                              #
    ################################

    # This block is not actually functional configuration,
    # but serves to illustrate the available configuration
    # options and document them in a way that is accessible
    # to users that `kubectl edit` this config map.
    #
    # These sample configuration options may be copied out of
    # this example block and unindented to be in the data block
    # to actually change the configuration.

    # leaseDuration is how long non-leaders will wait to try to acquire the
    # lock; 15 seconds is the value used by core kubernetes controllers.
    leaseDuration: "15s"

    # renewDeadline is how long a leader will try to renew the lease before
    # giving up; 10 seconds is the value used by core kubernetes controllers.
    renewDeadline: "10s"

    # retryPeriod is how long the leader election client waits between tries of
    # actions; 2 seconds is the value used by core kubernetes controllers.
    retryPeriod: "2s"

    # buckets is the number of buckets used to partition key space of each
    # Reconciler. If this number is M and the replica number of the controller
    # is N, the N replicas will compete for the M buckets. The owner of a
    # bucket will take care of the reconciling for the keys partitioned into
    # that bucket.
    buckets: "1"
kind: ConfigMap
metadata:
  annotations:
    knative.dev/example-checksum: 96896b00
  labels:
    serving.knative.dev/release: v0.22.1
  name: config-leader-election
  namespace: knative-serving
---
apiVersion: v1
data:
  _example: |
    ################################
    #                              #
    #    EXAMPLE CONFIGURATION     #
    #                              #
    ################################

    # This block is not actually functional configuration,
    # but serves to illustrate the available configuration
    # options and document them in a way that is accessible
    # to users that `kubectl edit` this config map.
    #
    # These sample configuration options may be copied out of
    # this example block and unindented to be in the data block
    # to actually change the configuration.

    # Common configuration for all Knative codebase
    zap-logger-config: |
      {
        "level": "info",
        "development": false,
        "outputPaths": ["stdout"],
        "errorOutputPaths": ["stderr"],
        "encoding": "json",
        "encoderConfig": {
          "timeKey": "timestamp",
          "levelKey": "severity",
          "nameKey": "logger",
          "callerKey": "caller",
          "messageKey": "message",
          "stacktraceKey": "stacktrace",
          "lineEnding": "",
          "levelEncoder": "",
          "timeEncoder": "iso8601",
          "durationEncoder": "",
          "callerEncoder": ""
        }
      }

    # Log level overrides
    # For all components except the queue proxy,
    # changes are picked up immediately.
    # For queue proxy, changes require recreation of the pods.
    loglevel.controller: "info"
    loglevel.autoscaler: "info"
    loglevel.queueproxy: "info"
    loglevel.webhook: "info"
    loglevel.activator: "info"
    loglevel.hpaautoscaler: "info"
    loglevel.certcontroller: "info"
    loglevel.istiocontroller: "info"
    loglevel.nscontroller: "info"
kind: ConfigMap
metadata:
  annotations:
    knative.dev/example-checksum: d9570453
  labels:
    serving.knative.dev/release: v0.22.1
  name: config-logging
  namespace: knative-serving
---
apiVersion: v1
data:
  _example: |
    ################################
    #                              #
    #    EXAMPLE CONFIGURATION     #
    #                              #
    ################################

    # This block is not actually functional configuration,
    # but serves to illustrate the available configuration
    # options and document them in a way that is accessible
    # to users that `kubectl edit` this config map.
    #
    # These sample configuration options may be copied out of
    # this example block and unindented to be in the data block
    # to actually change the configuration.

    # ingress.class specifies the default ingress class
    # to use when not dictated by Route annotation.
    #
    # If not specified, will use the Istio ingress.
    #
    # Note that changing the Ingress class of an existing Route
    # will result in undefined behavior.  Therefore it is best to only
    # update this value during the setup of Knative, to avoid getting
    # undefined behavior.
    ingress.class: "istio.ingress.networking.knative.dev"

    # certificate.class specifies the default Certificate class
    # to use when not dictated by Route annotation.
    #
    # If not specified, will use the Cert-Manager Certificate.
    #
    # Note that changing the Certificate class of an existing Route
    # will result in undefined behavior.  Therefore it is best to only
    # update this value during the setup of Knative, to avoid getting
    # undefined behavior.
    certificate.class: "cert-manager.certificate.networking.knative.dev"

    # domainTemplate specifies the golang text template string to use
    # when constructing the Knative service's DNS name. The default
    # value is "{{.Name}}.{{.Namespace}}.{{.Domain}}".
    #
    # Valid variables defined in the template include Name, Namespace, Domain,
    # Labels, and Annotations. Name will be the result of the tagTemplate
    # below, if a tag is specified for the route.
    #
    # Changing this value might be necessary when the extra levels in
    # the domain name generated is problematic for wildcard certificates
    # that only support a single level of domain name added to the
    # certificate's domain. In those cases you might consider using a value
    # of "{{.Name}}-{{.Namespace}}.{{.Domain}}", or removing the Namespace
    # entirely from the template. When choosing a new value be thoughtful
    # of the potential for conflicts - for example, when users choose to use
    # characters such as `-` in their service, or namespace, names.
    # {{.Annotations}} or {{.Labels}} can be used for any customization in the
    # go template if needed.
    # We strongly recommend keeping namespace part of the template to avoid
    # domain name clashes:
    # eg. '{{.Name}}-{{.Namespace}}.{{ index .Annotations "sub"}}.{{.Domain}}'
    # and you have an annotation {"sub":"foo"}, then the generated template
    # would be {Name}-{Namespace}.foo.{Domain}
    domainTemplate: "{{.Name}}.{{.Namespace}}.{{.Domain}}"

    # tagTemplate specifies the golang text template string to use
    # when constructing the DNS name for "tags" within the traffic blocks
    # of Routes and Configuration.  This is used in conjunction with the
    # domainTemplate above to determine the full URL for the tag.
    tagTemplate: "{{.Tag}}-{{.Name}}"

    # Controls whether TLS certificates are automatically provisioned and
    # installed in the Knative ingress to terminate external TLS connection.
    # 1. Enabled: enabling auto-TLS feature.
    # 2. Disabled: disabling auto-TLS feature.
    autoTLS: "Disabled"

    # Controls the behavior of the HTTP endpoint for the Knative ingress.
    # It requires autoTLS to be enabled.
    # 1. Enabled: The Knative ingress will be able to serve HTTP connection.
    # 2. Disabled: The Knative ingress will reject HTTP traffic.
    # 3. Redirected: The Knative ingress will send a 302 redirect for all
    # http connections, asking the clients to use HTTPS.
    httpProtocol: "Enabled"

    # rolloutDuration contains the minimal duration in seconds over which the
    # Configuration traffic targets are rolled out to the newest revision.
    rolloutDuration: "0"

    # autocreateClusterDomainClaims controls whether ClusterDomainClaims should
    # be automatically created (and deleted) as needed when DomainMappings are
    # reconciled.
    #
    # If this is "false", the cluster administrator is responsible for creating
    # ClusterDomainClaims and delegating them to namespaces via their
    # spec.Namespace field. This is useful for multitenant environments
    # which need to control which namespace can use a particular domain name in
    # a domain mapping.
    autocreateClusterDomainClaims: "true"

    # If true, networking plugins can add additional information to deployed
    # applications to make their pods directly accessible via their IPs even if mesh is
    # enabled and thus direct-addressability is usually not possible.
    # Consumers like Knative Serving can use this setting to adjust their behavior
    # accordingly, i.e. to drop fallback solutions for non-pod-addressable systems.
    #
    # NOTE: This flag is in an alpha state and is mostly here to enable internal testing
    #       for now. Use with caution.
    enable-mesh-pod-addressability: "false"
kind: ConfigMap
metadata:
  annotations:
    knative.dev/example-checksum: 15954d34
  labels:
    serving.knative.dev/release: v0.22.1
  name: config-network
  namespace: knative-serving
---
apiVersion: v1
data:
  _example: |
    ################################
    #                              #
    #    EXAMPLE CONFIGURATION     #
    #                              #
    ################################

    # This block is not actually functional configuration,
    # but serves to illustrate the available configuration
    # options and document them in a way that is accessible
    # to users that `kubectl edit` this config map.
    #
    # These sample configuration options may be copied out of
    # this example block and unindented to be in the data block
    # to actually change the configuration.

    # logging.enable-var-log-collection defaults to false.
    # The fluentd daemon set will be set up to collect /var/log if
    # this flag is true.
    logging.enable-var-log-collection: "false"

    # logging.revision-url-template provides a template to use for producing the
    # logging URL that is injected into the status of each Revision.
    logging.revision-url-template: "http://logging.example.com/?revisionUID=${REVISION_UID}"

    # If non-empty, this enables queue proxy writing user request logs to stdout, excluding probe
    # requests.
    # NB: after 0.18 release logging.enable-request-log must be explicitly set to true
    # in order for request logging to be enabled.
    #
    # The value determines the shape of the request logs and it must be a valid go text/template.
    # It is important to keep this as a single line. Multiple lines are parsed as separate entities
    # by most collection agents and will split the request logs into multiple records.
    #
    # The following fields and functions are available to the template:
    #
    # Request: An http.Request (see https://golang.org/pkg/net/http/#Request)
    # representing an HTTP request received by the server.
    #
    # Response:
    # struct {
    #   Code    int       // HTTP status code (see https://www.iana.org/assignments/http-status-codes/http-status-codes.xhtml)
    #   Size    int       // An int representing the size of the response.
    #   Latency float64   // A float64 representing the latency of the response in seconds.
    # }
    #
    # Revision:
    # struct {
    #   Name          string  // Knative revision name
    #   Namespace     string  // Knative revision namespace
    #   Service       string  // Knative service name
    #   Configuration string  // Knative configuration name
    #   PodName       string  // Name of the pod hosting the revision
    #   PodIP         string  // IP of the pod hosting the revision
    # }
    #
    logging.request-log-template: '{"httpRequest": {"requestMethod": "{{.Request.Method}}", "requestUrl": "{{js .Request.RequestURI}}", "requestSize": "{{.Request.ContentLength}}", "status": {{.Response.Code}}, "responseSize": "{{.Response.Size}}", "userAgent": "{{js .Request.UserAgent}}", "remoteIp": "{{js .Request.RemoteAddr}}", "serverIp": "{{.Revision.PodIP}}", "referer": "{{js .Request.Referer}}", "latency": "{{.Response.Latency}}s", "protocol": "{{.Request.Proto}}"}, "traceId": "{{index .Request.Header "X-B3-Traceid"}}"}'

    # If true, the request logging will be enabled.
    # NB: up to and including Knative version 0.18 if logging.request-log-template is non-empty, this value
    # will be ignored.
    logging.enable-request-log: "false"

    # If true, this enables queue proxy writing request logs for probe requests to stdout.
    # It uses the same template for user requests, i.e. logging.request-log-template.
    logging.enable-probe-request-log: "false"

    # metrics.backend-destination field specifies the system metrics destination.
    # It supports either prometheus (the default) or stackdriver.
    # Note: Using stackdriver will incur additional charges
    metrics.backend-destination: prometheus

    # metrics.request-metrics-backend-destination specifies the request metrics
    # destination. It enables queue proxy to send request metrics.
    # Currently supported values: prometheus (the default), stackdriver.
    metrics.request-metrics-backend-destination: prometheus

    # metrics.stackdriver-project-id field specifies the stackdriver project ID. This
    # field is optional. When running on GCE, application default credentials will be
    # used if this field is not provided.
    metrics.stackdriver-project-id: "<your stackdriver project id>"

    # metrics.allow-stackdriver-custom-metrics indicates whether it is allowed to send metrics to
    # Stackdriver using "global" resource type and custom metric type if the
    # metrics are not supported by "knative_revision" resource type. Setting this
    # flag to "true" could cause extra Stackdriver charge.
    # If metrics.backend-destination is not Stackdriver, this is ignored.
    metrics.allow-stackdriver-custom-metrics: "false"

    # profiling.enable indicates whether it is allowed to retrieve runtime profiling data from
    # the pods via an HTTP server in the format expected by the pprof visualization tool. When
    # enabled, the Knative Serving pods expose the profiling data on an alternate HTTP port 8008.
    # The HTTP context root for profiling is then /debug/pprof/.
    profiling.enable: "false"
kind: ConfigMap
metadata:
  annotations:
    knative.dev/example-checksum: 97c1d10b
  labels:
    serving.knative.dev/release: v0.22.1
  name: config-observability
  namespace: knative-serving
---
apiVersion: v1
data:
  _example: |
    ################################
    #                              #
    #    EXAMPLE CONFIGURATION     #
    #                              #
    ################################

    # This block is not actually functional configuration,
    # but serves to illustrate the available configuration
    # options and document them in a way that is accessible
    # to users that `kubectl edit` this config map.
    #
    # These sample configuration options may be copied out of
    # this example block and unindented to be in the data block
    # to actually change the configuration.
    #
    # This may be "zipkin" or "stackdriver", the default is "none"
    backend: "none"

    # URL to zipkin collector where traces are sent.
    # This must be specified when backend is "zipkin"
    zipkin-endpoint: "http://zipkin.istio-system.svc.cluster.local:9411/api/v2/spans"

    # The GCP project into which stackdriver metrics will be written
    # when backend is "stackdriver".  If unspecified, the project-id
    # is read from GCP metadata when running on GCP.
    stackdriver-project-id: "my-project"

    # Enable zipkin debug mode. This allows all spans to be sent to the server
    # bypassing sampling.
    debug: "false"

    # Percentage (0-1) of requests to trace
    sample-rate: "0.1"
kind: ConfigMap
metadata:
  annotations:
    knative.dev/example-checksum: 4002b4c2
  labels:
    serving.knative.dev/release: v0.22.1
  name: config-tracing
  namespace: knative-serving
---
apiVersion: v1
kind: Secret
metadata:
  labels:
    networking.knative.dev/ingress-provider: istio
    serving.knative.dev/release: v0.22.1
  name: istio-webhook-certs
  namespace: knative-serving
---
apiVersion: v1
kind: Secret
metadata:
  labels:
    serving.knative.dev/release: v0.22.1
  name: webhook-certs
  namespace: knative-serving
---
apiVersion: v1
kind: Service
metadata:
  labels:
    experimental.istio.io/disable-gateway-port-translation: "true"
    networking.knative.dev/ingress-provider: istio
    serving.knative.dev/release: v0.22.1
  name: knative-local-gateway
  namespace: istio-system
spec:
  ports:
  - name: http2
    port: 80
    targetPort: 8081
  selector:
    app: cluster-local-gateway
    istio: cluster-local-gateway
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: activator
    serving.knative.dev/release: v0.22.1
  name: activator-service
  namespace: knative-serving
spec:
  ports:
  - name: http-metrics
    port: 9090
    targetPort: 9090
  - name: http-profiling
    port: 8008
    targetPort: 8008
  - name: http
    port: 80
    targetPort: 8012
  - name: http2
    port: 81
    targetPort: 8013
  selector:
    app: activator
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: autoscaler
    serving.knative.dev/release: v0.22.1
  name: autoscaler
  namespace: knative-serving
spec:
  ports:
  - name: http-metrics
    port: 9090
    targetPort: 9090
  - name: http-profiling
    port: 8008
    targetPort: 8008
  - name: http
    port: 8080
    targetPort: 8080
  selector:
    app: autoscaler
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: controller
    serving.knative.dev/release: v0.22.1
  name: controller
  namespace: knative-serving
spec:
  ports:
  - name: http-metrics
    port: 9090
    targetPort: 9090
  - name: http-profiling
    port: 8008
    targetPort: 8008
  selector:
    app: controller
---
apiVersion: v1
kind: Service
metadata:
  labels:
    networking.knative.dev/ingress-provider: istio
    role: istio-webhook
    serving.knative.dev/release: v0.22.1
  name: istio-webhook
  namespace: knative-serving
spec:
  ports:
  - name: http-metrics
    port: 9090
    targetPort: 9090
  - name: http-profiling
    port: 8008
    targetPort: 8008
  - name: https-webhook
    port: 443
    targetPort: 8443
  selector:
    app: istio-webhook
---
apiVersion: v1
kind: Service
metadata:
  labels:
    role: webhook
    serving.knative.dev/release: v0.22.1
  name: webhook
  namespace: knative-serving
spec:
  ports:
  - name: http-metrics
    port: 9090
    targetPort: 9090
  - name: http-profiling
    port: 8008
    targetPort: 8008
  - name: https-webhook
    port: 443
    targetPort: 8443
  selector:
    role: webhook
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    serving.knative.dev/release: v0.22.1
  name: activator
  namespace: knative-serving
spec:
  selector:
    matchLabels:
      app: activator
      role: activator
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "false"
        sidecar.istio.io/inject: "true"
      labels:
        app: activator
        role: activator
        serving.knative.dev/release: v0.22.1
    spec:
      containers:
      - env:
        - name: GOGC
          value: "500"
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: SYSTEM_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: CONFIG_LOGGING_NAME
          value: config-logging
        - name: CONFIG_OBSERVABILITY_NAME
          value: config-observability
        - name: METRICS_DOMAIN
          value: knative.dev/internal/serving
        image: registry.cn-guangzhou.aliyuncs.com/k8s-docker-panyx/knative-dev_serving_cmd_activator:fed92af8b977
        livenessProbe:
          failureThreshold: 12
          httpGet:
            httpHeaders:
            - name: k-kubelet-probe
              value: activator
            port: 8012
          initialDelaySeconds: 15
        name: activator
        ports:
        - containerPort: 9090
          name: metrics
        - containerPort: 8008
          name: profiling
        - containerPort: 8012
          name: http1
        - containerPort: 8013
          name: h2c
        readinessProbe:
          failureThreshold: 12
          httpGet:
            httpHeaders:
            - name: k-kubelet-probe
              value: activator
            port: 8012
        resources:
          limits:
            cpu: 1000m
            memory: 600Mi
          requests:
            cpu: 300m
            memory: 60Mi
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - all
          readOnlyRootFilesystem: true
          runAsNonRoot: true
      serviceAccountName: controller
      terminationGracePeriodSeconds: 600
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    serving.knative.dev/release: v0.22.1
  name: autoscaler
  namespace: knative-serving
spec:
  replicas: 1
  selector:
    matchLabels:
      app: autoscaler
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "false"
        sidecar.istio.io/inject: "true"
      labels:
        app: autoscaler
        serving.knative.dev/release: v0.22.1
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: autoscaler
              topologyKey: kubernetes.io/hostname
            weight: 100
      containers:
      - env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: SYSTEM_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: CONFIG_LOGGING_NAME
          value: config-logging
        - name: CONFIG_OBSERVABILITY_NAME
          value: config-observability
        - name: METRICS_DOMAIN
          value: knative.dev/serving
        image: registry.cn-guangzhou.aliyuncs.com/k8s-docker-panyx/knative-dev_serving_cmd_autoscaler:bc5ae3090ab0
        livenessProbe:
          failureThreshold: 6
          httpGet:
            httpHeaders:
            - name: k-kubelet-probe
              value: autoscaler
            port: 8080
        name: autoscaler
        ports:
        - containerPort: 9090
          name: metrics
        - containerPort: 8008
          name: profiling
        - containerPort: 8080
          name: websocket
        readinessProbe:
          httpGet:
            httpHeaders:
            - name: k-kubelet-probe
              value: autoscaler
            port: 8080
        resources:
          limits:
            cpu: 1000m
            memory: 1000Mi
          requests:
            cpu: 100m
            memory: 100Mi
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - all
          readOnlyRootFilesystem: true
          runAsNonRoot: true
      serviceAccountName: controller
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    serving.knative.dev/release: v0.22.1
  name: controller
  namespace: knative-serving
spec:
  selector:
    matchLabels:
      app: controller
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        sidecar.istio.io/inject: "true"
      labels:
        app: controller
        serving.knative.dev/release: v0.22.1
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: controller
              topologyKey: kubernetes.io/hostname
            weight: 100
      containers:
      - env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: SYSTEM_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: CONFIG_LOGGING_NAME
          value: config-logging
        - name: CONFIG_OBSERVABILITY_NAME
          value: config-observability
        - name: METRICS_DOMAIN
          value: knative.dev/internal/serving
        image: registry.cn-guangzhou.aliyuncs.com/k8s-docker-panyx/knative-dev_serving_cmd_controller:bd7c6350e5d5
        name: controller
        ports:
        - containerPort: 9090
          name: metrics
        - containerPort: 8008
          name: profiling
        resources:
          limits:
            cpu: 1000m
            memory: 1000Mi
          requests:
            cpu: 100m
            memory: 100Mi
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - all
          readOnlyRootFilesystem: true
          runAsNonRoot: true
      serviceAccountName: controller
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    networking.knative.dev/ingress-provider: istio
    serving.knative.dev/release: v0.22.1
  name: istio-webhook
  namespace: knative-serving
spec:
  selector:
    matchLabels:
      app: istio-webhook
      role: istio-webhook
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "false"
        sidecar.istio.io/inject: "true"
      labels:
        app: istio-webhook
        role: istio-webhook
        serving.knative.dev/release: v0.22.1
    spec:
      containers:
      - env:
        - name: SYSTEM_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: CONFIG_LOGGING_NAME
          value: config-logging
        - name: CONFIG_OBSERVABILITY_NAME
          value: config-observability
        - name: METRICS_DOMAIN
          value: knative.dev/net-istio
        - name: WEBHOOK_NAME
          value: istio-webhook
        image: registry.cn-guangzhou.aliyuncs.com/k8s-docker-panyx/knative-dev_net-istio_cmd_webhook:1e371db6b1a9
        name: webhook
        ports:
        - containerPort: 9090
          name: metrics
        - containerPort: 8008
          name: profiling
        - containerPort: 8443
          name: https-webhook
        resources:
          limits:
            cpu: 200m
            memory: 200Mi
          requests:
            cpu: 20m
            memory: 20Mi
        securityContext:
          allowPrivilegeEscalation: false
      serviceAccountName: controller
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    networking.knative.dev/ingress-provider: istio
    serving.knative.dev/release: v0.22.1
  name: networking-istio
  namespace: knative-serving
spec:
  selector:
    matchLabels:
      app: networking-istio
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        sidecar.istio.io/inject: "true"
      labels:
        app: networking-istio
        serving.knative.dev/release: v0.22.1
    spec:
      containers:
      - env:
        - name: SYSTEM_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: CONFIG_LOGGING_NAME
          value: config-logging
        - name: CONFIG_OBSERVABILITY_NAME
          value: config-observability
        - name: METRICS_DOMAIN
          value: knative.dev/net-istio
        image: registry.cn-guangzhou.aliyuncs.com/k8s-docker-panyx/knative-dev_net-istio_cmd_controller:ff8680da52ef
        name: networking-istio
        ports:
        - containerPort: 9090
          name: metrics
        - containerPort: 8008
          name: profiling
        resources:
          limits:
            cpu: 300m
            memory: 400Mi
          requests:
            cpu: 30m
            memory: 40Mi
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - all
          readOnlyRootFilesystem: true
          runAsNonRoot: true
      serviceAccountName: controller
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    serving.knative.dev/release: v0.22.1
  name: webhook
  namespace: knative-serving
spec:
  selector:
    matchLabels:
      app: webhook
      role: webhook
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "false"
        sidecar.istio.io/inject: "true"
      labels:
        app: webhook
        role: webhook
        serving.knative.dev/release: v0.22.1
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: webhook
              topologyKey: kubernetes.io/hostname
            weight: 100
      containers:
      - env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: SYSTEM_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: CONFIG_LOGGING_NAME
          value: config-logging
        - name: CONFIG_OBSERVABILITY_NAME
          value: config-observability
        - name: WEBHOOK_NAME
          value: webhook
        - name: WEBHOOK_PORT
          value: "8443"
        - name: METRICS_DOMAIN
          value: knative.dev/internal/serving
        image: registry.cn-guangzhou.aliyuncs.com/k8s-docker-panyx/knative-dev_serving_cmd_webhook:6f41d379f1aa
        livenessProbe:
          failureThreshold: 6
          httpGet:
            httpHeaders:
            - name: k-kubelet-probe
              value: webhook
            port: 8443
            scheme: HTTPS
          initialDelaySeconds: 20
          periodSeconds: 1
        name: webhook
        ports:
        - containerPort: 9090
          name: metrics
        - containerPort: 8008
          name: profiling
        - containerPort: 8443
          name: https-webhook
        readinessProbe:
          httpGet:
            httpHeaders:
            - name: k-kubelet-probe
              value: webhook
            port: 8443
            scheme: HTTPS
          periodSeconds: 1
        resources:
          limits:
            cpu: 500m
            memory: 500Mi
          requests:
            cpu: 100m
            memory: 100Mi
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - all
          readOnlyRootFilesystem: true
          runAsNonRoot: true
      serviceAccountName: controller
      terminationGracePeriodSeconds: 300
---
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  labels:
    serving.knative.dev/release: v0.22.1
  name: activator-pdb
  namespace: knative-serving
spec:
  minAvailable: 80%
  selector:
    matchLabels:
      app: activator
---
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  labels:
    serving.knative.dev/release: v0.22.1
  name: webhook-pdb
  namespace: knative-serving
spec:
  minAvailable: 80%
  selector:
    matchLabels:
      app: webhook
---
apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  labels:
    serving.knative.dev/release: v0.22.1
  name: activator
  namespace: knative-serving
spec:
  maxReplicas: 20
  metrics:
  - resource:
      name: cpu
      target:
        averageUtilization: 100
        type: Utilization
    type: Resource
  minReplicas: 1
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: activator
---
apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  labels:
    serving.knative.dev/release: v0.22.1
  name: webhook
  namespace: knative-serving
spec:
  maxReplicas: 5
  metrics:
  - resource:
      name: cpu
      target:
        averageUtilization: 100
        type: Utilization
    type: Resource
  minReplicas: 1
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: webhook
---
apiVersion: caching.internal.knative.dev/v1alpha1
kind: Image
metadata:
  labels:
    serving.knative.dev/release: v0.22.1
  name: queue-proxy
  namespace: knative-serving
spec:
  image: registry.cn-guangzhou.aliyuncs.com/k8s-docker-panyx/knative-dev_serving_cmd_queue:0b8e03117035
---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: knative
  namespace: knative-serving
spec:
  host: '*.knative-serving.svc.cluster.local'
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
---
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  labels:
    networking.knative.dev/ingress-provider: istio
    serving.knative.dev/release: v0.22.1
  name: knative-local-gateway
  namespace: knative-serving
spec:
  selector:
    app: cluster-local-gateway
    istio: cluster-local-gateway
  servers:
  - hosts:
    - '*'
    port:
      name: http
      number: 8081
      protocol: HTTP
---
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: activator-service
  namespace: knative-serving
spec:
  action: ALLOW
  rules:
  - {}
  selector:
    matchLabels:
      app: activator
---
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: autoscaler
  namespace: knative-serving
spec:
  action: ALLOW
  rules:
  - {}
  selector:
    matchLabels:
      app: autoscaler
---
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: controller
  namespace: knative-serving
spec:
  action: ALLOW
  rules:
  - {}
  selector:
    matchLabels:
      app: controller
---
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: istio-webhook
  namespace: knative-serving
spec:
  action: ALLOW
  rules:
  - {}
  selector:
    matchLabels:
      app: istio-webhook
---
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: webhook
  namespace: knative-serving
spec:
  action: ALLOW
  rules:
  - {}
  selector:
    matchLabels:
      role: webhook
---
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  labels:
    networking.knative.dev/ingress-provider: istio
    serving.knative.dev/release: v0.22.1
  name: domainmapping-webhook
  namespace: knative-serving
spec:
  portLevelMtls:
    "8443":
      mode: PERMISSIVE
  selector:
    matchLabels:
      app: domainmapping-webhook
---
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  labels:
    networking.knative.dev/ingress-provider: istio
    serving.knative.dev/release: v0.22.1
  name: istio-webhook
  namespace: knative-serving
spec:
  portLevelMtls:
    "8443":
      mode: PERMISSIVE
  selector:
    matchLabels:
      app: istio-webhook
---
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  labels:
    networking.knative.dev/ingress-provider: istio
    serving.knative.dev/release: v0.22.1
  name: webhook
  namespace: knative-serving
spec:
  portLevelMtls:
    "8443":
      mode: PERMISSIVE
  selector:
    matchLabels:
      app: webhook
---
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  labels:
    networking.knative.dev/ingress-provider: istio
    serving.knative.dev/release: v0.22.1
  name: config.webhook.istio.networking.internal.knative.dev
webhooks:
- admissionReviewVersions:
  - v1beta1
  clientConfig:
    service:
      name: istio-webhook
      namespace: knative-serving
  failurePolicy: Fail
  name: config.webhook.istio.networking.internal.knative.dev
  namespaceSelector:
    matchExpressions:
    - key: serving.knative.dev/release
      operator: Exists
  sideEffects: None
---
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  labels:
    serving.knative.dev/release: v0.22.1
  name: config.webhook.serving.knative.dev
webhooks:
- admissionReviewVersions:
  - v1
  - v1beta1
  clientConfig:
    service:
      name: webhook
      namespace: knative-serving
  failurePolicy: Fail
  name: config.webhook.serving.knative.dev
  namespaceSelector:
    matchExpressions:
    - key: serving.knative.dev/release
      operator: Exists
  sideEffects: None
  timeoutSeconds: 10
---
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  labels:
    serving.knative.dev/release: v0.22.1
  name: validation.webhook.serving.knative.dev
webhooks:
- admissionReviewVersions:
  - v1
  - v1beta1
  clientConfig:
    service:
      name: webhook
      namespace: knative-serving
  failurePolicy: Fail
  name: validation.webhook.serving.knative.dev
  sideEffects: None
  timeoutSeconds: 10
